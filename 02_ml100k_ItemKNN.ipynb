{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1d988e-9aab-47ac-9078-624e6fb12f18",
   "metadata": {},
   "source": [
    "## Model: ItemKNN (Item-based Top-N Recommendation)\n",
    "\n",
    "This notebook implements the **ItemKNN** algorithm based on the following foundational paper:\n",
    "\n",
    "> Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl.  \n",
    "> **Item-based Collaborative Filtering Recommendation Algorithms**  \n",
    "> WWW 2001.  \n",
    "> [PDF](https://www.cs.umn.edu/research/GroupLens/publications/webconf.pdf)\n",
    "\n",
    "### Core idea\n",
    "ItemKNN builds a similarity matrix between items based on historical user interactions and uses it to recommend items that are most similar to those a user has previously interacted with.\n",
    "\n",
    "### Implementation notes\n",
    "- Similarities are computed using cosine similarity over the user-item interaction matrix.\n",
    "- For each user, the top-N items are recommended by aggregating scores from similar items the user has interacted with.\n",
    "- No training is needed beyond similarity computation.\n",
    "\n",
    "This implementation matches the methodology described in the original paper and enables integration with re-ranking and evaluation pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc06c51-b131-4eb3-a411-8f1429ad173e",
   "metadata": {},
   "source": [
    "### 📊 Comparison with RecBole: ItemKNN Implementation\n",
    "\n",
    "This notebook implements the ItemKNN algorithm based on *Deshpande & Karypis (2004)*. While the core idea—neighborhood-based item similarity—is consistent, several architectural and evaluation differences exist compared to the implementation in RecBole.\n",
    "\n",
    "#### 🔍 Metric Comparison (ML-100K, Original ItemKNN Only)\n",
    "\n",
    "| Metric       | RecBole KNN | Custom KNN | Δ (%)        |\n",
    "|--------------|-------------|------------|--------------|\n",
    "| NDCG@10      | 0.3167      | 0.2647     | –16.43%      |\n",
    "| Precision@10 | 0.1813      | 0.2074     | **+14.40%**  |\n",
    "| Recall@10    | 0.2118      | 0.1843     | –12.98%      |\n",
    "| Gini Index   | 0.9214      | 0.6871     | –25.42%      |\n",
    "| Item Coverage| 0.2656      | 0.2212     | –16.72%      |\n",
    "| Entropy      | 0.0119      | 0.6362     | **+5245%**   |\n",
    "| Tail %       | 0.0         | 0.0        | –            |\n",
    "\n",
    "> *Note: RecBole's entropy appears artificially low, likely due to the metric being calculated over the ranking positions rather than raw item occurrence distributions.*\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of Deviations from RecBole\n",
    "\n",
    "The following technical differences likely explain the evaluation deviations between this implementation and RecBole’s built-in `ItemKNN`:\n",
    "\n",
    "- **Similarity Function**  \n",
    "  This notebook uses **cosine similarity** on the binary interaction matrix.  \n",
    "  RecBole supports additional metrics like **Asymmetric Cosine**, **Pearson**, or **Jaccard**, and allows fine-tuning similarity behavior via config files.\n",
    "\n",
    "- **Score Normalization**  \n",
    "  Item relevance scores are not normalized in this code.  \n",
    "  RecBole optionally normalizes similarity scores (e.g., dividing by sum of similarities or user profile length).\n",
    "\n",
    "- **Neighbor Filtering**  \n",
    "  The implementation retains only top-`k` neighbors with **positive similarity**, excluding weak links.  \n",
    "  RecBole may include additional logic to ensure numerical stability or handle cold-starts.\n",
    "\n",
    "- **User-based Filtering / Preprocessing**  \n",
    "  RecBole filters users/items below a configurable threshold (`user_inter_num`, `item_inter_num`), leading to different evaluation sets.  \n",
    "  Here, all data is used without such filtering.\n",
    "\n",
    "- **Evaluation Methodology**  \n",
    "  RecBole performs **full ranking over all items** not in the training set.  \n",
    "  This notebook computes similarity scores from neighbors only and builds recommendation lists accordingly.\n",
    "\n",
    "- **Relevance Scoring in NDCG**  \n",
    "  This code uses **actual rating values** (1–5) as graded relevance in DCG, while RecBole uses **binary relevance** (1 if item in ground truth, else 0) unless explicitly configured otherwise.\n",
    "\n",
    "- **Data Splitting Strategy**  \n",
    "  This notebook uses a stratified 80/20 random split.  \n",
    "  RecBole supports split strategies such as **`leave_one_out`**, **`user_leave_one_out`**, and **chronological split**, which affect ground truth and negative sampling.\n",
    "\n",
    "- **Reranking Compatibility**  \n",
    "  This implementation separates recommendation from reranking entirely, which allows flexible use of algorithms like **MMR** or **simple diversification**.  \n",
    "  RecBole does not support custom rerankers in its evaluation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While differences exist, this implementation follows the conceptual design from the original ItemKNN paper and enables full control over similarity computation and reranking. The observed deviations in evaluation metrics (especially diversity) stem from simplifications, absence of internal filtering logic, and more flexible use of graded relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ff12d6-dff6-4066-9b76-5fe257aae5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k=10)\n",
      "================================================================================\n",
      "\n",
      "Loading MovieLens 100K dataset...\n",
      "Splitting data for evaluation...\n",
      "Creating user-item matrix...\n",
      "\n",
      "Training ItemKNN model...\n",
      "Computing item-item similarity matrix...\n",
      "\n",
      "Initializing rerankers...\n",
      "\n",
      "Evaluating 943 users...\n",
      "\n",
      "Evaluating Original ItemKNN...\n",
      "\n",
      "Evaluating Simple Reranker...\n",
      "\n",
      "Evaluating MMR Reranker...\n",
      "\n",
      "============================== ACCURACY METRICS COMPARISON ==============================\n",
      "Metric         Original ItemKNN    Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "ndcg@10        0.2736               0.2735 (-0.1%)     0.2775 (+1.4%)     \n",
      "precision@10   0.2888               0.2889 (+0.0%)     0.2940 (+1.8%)     \n",
      "recall@10      0.1878               0.1880 (+0.1%)     0.1921 (+2.3%)     \n",
      "\n",
      "============================== DIVERSITY METRICS COMPARISON ==============================\n",
      "Metric         Original ItemKNN    Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "item_coverage  0.1129               0.1147 (+1.6%)     0.1232 (+9.1%)     \n",
      "gini_index     0.6791               0.6811 (+0.3%)     0.6889 (+1.4%)     \n",
      "shannon_entropy0.5925               0.5939 (+0.2%)     0.6002 (+1.3%)     \n",
      "tail_percentage0.0000               0.0000 (+inf%)     0.0004 (+inf%)     \n",
      "\n",
      "============================== METRIC INTERPRETATIONS ==============================\n",
      "Accuracy Metrics:\n",
      "- NDCG: Higher is better, measures ranking quality\n",
      "- Precision: Higher is better, measures relevant item ratio in recommendations\n",
      "- Recall: Higher is better, measures coverage of all relevant items\n",
      "\n",
      "Diversity Metrics:\n",
      "- Item Coverage: Higher means more catalog items are recommended\n",
      "- Gini Index: Lower means more equality in item recommendations\n",
      "- Shannon Entropy: Higher means more diverse recommendations\n",
      "- Tail Percentage: Higher means more niche items are recommended\n"
     ]
    }
   ],
   "source": [
    "# ItemKNN Recommender with Diversity Reranking\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#################################\n",
    "# ITEMKNN RECOMMENDER IMPLEMENTATION\n",
    "#################################\n",
    "\n",
    "class ItemKNNRecommender:\n",
    "    def __init__(self, k=50, random_state=42):\n",
    "        \"\"\"\n",
    "        Item-Based K-Nearest Neighbors recommender algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        - k: number of nearest neighbors to consider\n",
    "        - random_state: seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Train the ItemKNN model on the user-item matrix\n",
    "        \n",
    "        Parameters:\n",
    "        - user_item_matrix: scipy sparse matrix with user-item interactions\n",
    "        \n",
    "        Returns:\n",
    "        - self\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.n_users, self.n_items = user_item_matrix.shape\n",
    "        \n",
    "        # Compute item-item similarity matrix\n",
    "        print(\"Computing item-item similarity matrix...\")\n",
    "        self.item_factors = cosine_similarity(user_item_matrix.T)\n",
    "        \n",
    "        # Zero out self-similarity to avoid recommending the same item\n",
    "        np.fill_diagonal(self.item_factors, 0)\n",
    "        \n",
    "        # Create a dictionary of items each user has interacted with\n",
    "        self.user_items = defaultdict(set)\n",
    "        for user, item in zip(*self.user_item_matrix.nonzero()):\n",
    "            self.user_items[user].add(item)\n",
    "        \n",
    "        # For each item, find its k nearest neighbors\n",
    "        self.item_neighbors = {}\n",
    "        for item_id in range(self.n_items):\n",
    "            # Get similarity scores for this item\n",
    "            similarities = self.item_factors[item_id]\n",
    "            \n",
    "            # Find the k most similar items\n",
    "            neighbor_ids = np.argsort(similarities)[::-1][:self.k]\n",
    "            \n",
    "            # Store neighbors with their similarity scores\n",
    "            self.item_neighbors[item_id] = {\n",
    "                neighbor_id: similarities[neighbor_id] \n",
    "                for neighbor_id in neighbor_ids \n",
    "                if similarities[neighbor_id] > 0\n",
    "            }\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def recommend(self, user_id, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate item recommendations for a user\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index\n",
    "        - n: number of recommendations to generate\n",
    "        - exclude_seen: whether to exclude items the user has already interacted with\n",
    "        \n",
    "        Returns:\n",
    "        - list of n recommended item indices\n",
    "        \"\"\"\n",
    "        # If the user has no interactions, return random recommendations\n",
    "        if user_id not in self.user_items or not self.user_items[user_id]:\n",
    "            # Get random recommendations\n",
    "            all_items = set(range(self.n_items))\n",
    "            seen_items = self.user_items.get(user_id, set())\n",
    "            candidate_items = list(all_items - seen_items) if exclude_seen else list(all_items)\n",
    "            \n",
    "            # Return random items\n",
    "            if len(candidate_items) <= n:\n",
    "                return candidate_items\n",
    "            return random.sample(candidate_items, n)\n",
    "        \n",
    "        # Initialize score accumulator for all items\n",
    "        scores = np.zeros(self.n_items)\n",
    "        \n",
    "        # For each item the user has interacted with\n",
    "        for item_id in self.user_items[user_id]:\n",
    "            # Get its neighbors\n",
    "            if item_id in self.item_neighbors:\n",
    "                # Add similarity scores to the accumulator\n",
    "                for neighbor_id, similarity in self.item_neighbors[item_id].items():\n",
    "                    scores[neighbor_id] += similarity\n",
    "        \n",
    "        # If requested, exclude items the user has already interacted with\n",
    "        if exclude_seen:\n",
    "            for item_id in self.user_items[user_id]:\n",
    "                scores[item_id] = -np.inf\n",
    "                \n",
    "        # Get top n items by score\n",
    "        top_items = np.argsort(scores)[::-1][:n]\n",
    "        \n",
    "        return top_items\n",
    "\n",
    "#################################\n",
    "# RERANKER IMPLEMENTATION\n",
    "#################################\n",
    "\n",
    "class SimpleReranker:\n",
    "    \"\"\"\n",
    "    Simple reranker that balances original scores with diversity\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Initialize reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained recommender model\n",
    "        - alpha: weight for original scores (between 0 and 1)\n",
    "                 higher alpha means more focus on accuracy\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Calculate item popularity\n",
    "        self.item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    self.item_popularity[item] += 1\n",
    "        \n",
    "        # Normalize popularity\n",
    "        max_pop = np.max(self.item_popularity)\n",
    "        if max_pop > 0:\n",
    "            self.norm_popularity = self.item_popularity / max_pop\n",
    "        else:\n",
    "            self.norm_popularity = np.zeros_like(self.item_popularity)\n",
    "    \n",
    "    def rerank(self, user_id, n=10):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations\n",
    "        \"\"\"\n",
    "        # Get original recommendations as a larger candidate pool\n",
    "        candidates = self.model.recommend(user_id, n=n*3, exclude_seen=True)\n",
    "        \n",
    "        # Calculate original scores for these candidates\n",
    "        scores = np.zeros(self.model.n_items)\n",
    "        \n",
    "        # For each item the user has interacted with\n",
    "        for item_id in self.model.user_items.get(user_id, []):\n",
    "            # Get its neighbors\n",
    "            if item_id in self.model.item_neighbors:\n",
    "                # Add similarity scores to the accumulator\n",
    "                for neighbor_id, similarity in self.model.item_neighbors[item_id].items():\n",
    "                    scores[neighbor_id] += similarity\n",
    "        \n",
    "        # Initialize selected items\n",
    "        selected = []\n",
    "        \n",
    "        # Iteratively select items\n",
    "        while len(selected) < n and candidates.size > 0:\n",
    "            best_score = -np.inf\n",
    "            best_item = None\n",
    "            \n",
    "            for item in candidates:\n",
    "                if item in selected:\n",
    "                    continue\n",
    "                \n",
    "                # Original score component\n",
    "                score_orig = scores[item]\n",
    "                \n",
    "                # Diversity component\n",
    "                diversity_score = 0\n",
    "                if selected:\n",
    "                    # Use item factors to calculate similarity\n",
    "                    item_factors = self.model.item_factors[item]\n",
    "                    selected_factors = self.model.item_factors[selected]\n",
    "                    \n",
    "                    # Calculate average similarity\n",
    "                    similarities = []\n",
    "                    for i, sel_idx in enumerate(selected):\n",
    "                        # Get similarity directly from the matrix\n",
    "                        sim = self.model.item_factors[item, sel_idx]\n",
    "                        similarities.append(sim)\n",
    "                    \n",
    "                    if similarities:\n",
    "                        avg_sim = np.mean(similarities)\n",
    "                        diversity_score = 1 - avg_sim\n",
    "                \n",
    "                # Novelty component (inverse popularity)\n",
    "                novelty_score = 1 - self.norm_popularity[item]\n",
    "                \n",
    "                # Calculate weighted score\n",
    "                combined_score = (\n",
    "                    self.alpha * score_orig + \n",
    "                    (1 - self.alpha) * 0.5 * diversity_score + \n",
    "                    (1 - self.alpha) * 0.5 * novelty_score\n",
    "                )\n",
    "                \n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_item = item\n",
    "            \n",
    "            if best_item is None:\n",
    "                break\n",
    "                \n",
    "            selected.append(best_item)\n",
    "            candidates = candidates[candidates != best_item]\n",
    "            \n",
    "        return selected\n",
    "\n",
    "class MMRReranker:\n",
    "    \"\"\"\n",
    "    Maximum Marginal Relevance (MMR) Reranker\n",
    "    \n",
    "    This reranker balances between relevance and diversity explicitly by\n",
    "    selecting items that maximize marginal relevance - items that are\n",
    "    both relevant to the user and different from already selected items.\n",
    "    \n",
    "    MMR formula: MMR = λ * rel(i) - (1-λ) * max(sim(i,j)) for j in selected items\n",
    "    \n",
    "    Where:\n",
    "    - rel(i) is the relevance of item i to the user\n",
    "    - sim(i,j) is the similarity between items i and j\n",
    "    - λ is a parameter that controls the trade-off between relevance and diversity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, lambda_param=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the MMR reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained recommender model\n",
    "        - lambda_param: trade-off parameter between relevance and diversity (0-1)\n",
    "                        higher values favor relevance, lower values favor diversity\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.lambda_param = lambda_param\n",
    "        \n",
    "    def calculate_item_similarity(self, item1, item2):\n",
    "        \"\"\"\n",
    "        Calculate similarity between two items\n",
    "        \n",
    "        Parameters:\n",
    "        - item1: index of first item\n",
    "        - item2: index of second item\n",
    "        \n",
    "        Returns:\n",
    "        - similarity: similarity between items (0 to 1)\n",
    "        \"\"\"\n",
    "        # For ItemKNN, we can directly use the precomputed similarity matrix\n",
    "        return self.model.item_factors[item1, item2]\n",
    "    \n",
    "    def rerank(self, user_id, n=10, candidate_size=100):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations using Maximum Marginal Relevance\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index in the model\n",
    "        - n: number of recommendations to return\n",
    "        - candidate_size: number of initial candidates to consider\n",
    "        \n",
    "        Returns:\n",
    "        - reranked_items: list of reranked item indices\n",
    "        \"\"\"\n",
    "        # Get original candidates and scores\n",
    "        candidates = self.model.recommend(user_id, n=candidate_size, exclude_seen=True)\n",
    "        \n",
    "        # Calculate relevance scores for these candidates\n",
    "        relevance_scores = np.zeros(self.model.n_items)\n",
    "        \n",
    "        # For each item the user has interacted with\n",
    "        for item_id in self.model.user_items.get(user_id, []):\n",
    "            # Get its neighbors\n",
    "            if item_id in self.model.item_neighbors:\n",
    "                # Add similarity scores to the accumulator\n",
    "                for neighbor_id, similarity in self.model.item_neighbors[item_id].items():\n",
    "                    relevance_scores[neighbor_id] += similarity\n",
    "        \n",
    "        # Normalize relevance scores to [0,1] range for the candidates\n",
    "        candidate_scores = relevance_scores[candidates]\n",
    "        min_score = np.min(candidate_scores)\n",
    "        max_score = np.max(candidate_scores)\n",
    "        score_range = max_score - min_score\n",
    "        \n",
    "        if score_range > 0:\n",
    "            normalized_scores = (candidate_scores - min_score) / score_range\n",
    "        else:\n",
    "            normalized_scores = np.zeros_like(candidate_scores)\n",
    "        \n",
    "        # Initialize selected items\n",
    "        selected = []\n",
    "        \n",
    "        # Select first item (most relevant)\n",
    "        if candidates.size > 0:\n",
    "            selected.append(candidates[np.argmax(normalized_scores)])\n",
    "            remaining_candidates = set(candidates) - set(selected)\n",
    "        else:\n",
    "            remaining_candidates = set()\n",
    "        \n",
    "        # Iteratively select items using MMR\n",
    "        while len(selected) < n and remaining_candidates:\n",
    "            max_mmr = -np.inf\n",
    "            max_item = None\n",
    "            \n",
    "            for item in remaining_candidates:\n",
    "                # Get relevance component\n",
    "                item_idx = np.where(candidates == item)[0][0]\n",
    "                relevance = normalized_scores[item_idx]\n",
    "                \n",
    "                # Calculate diversity component (inverse of maximum similarity)\n",
    "                max_sim = 0\n",
    "                for selected_item in selected:\n",
    "                    sim = self.calculate_item_similarity(item, selected_item)\n",
    "                    max_sim = max(max_sim, sim)\n",
    "                \n",
    "                # Calculate MMR score\n",
    "                mmr_score = self.lambda_param * relevance - (1 - self.lambda_param) * max_sim\n",
    "                \n",
    "                if mmr_score > max_mmr:\n",
    "                    max_mmr = mmr_score\n",
    "                    max_item = item\n",
    "            \n",
    "            if max_item is not None:\n",
    "                selected.append(max_item)\n",
    "                remaining_candidates.remove(max_item)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return selected\n",
    "\n",
    "#################################\n",
    "# EVALUATION METRICS\n",
    "#################################\n",
    "\n",
    "def calculate_ndcg(recommended_items, relevant_items, relevant_scores, k=None):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(recommended_items)\n",
    "    else:\n",
    "        k = min(k, len(recommended_items))\n",
    "    \n",
    "    # Create a dictionary mapping relevant items to their scores\n",
    "    relevance_map = {item_id: score for item_id, score in zip(relevant_items, relevant_scores)}\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = 0\n",
    "    for i, item_id in enumerate(recommended_items[:k]):\n",
    "        if item_id in relevance_map:\n",
    "            # Use rating as relevance score\n",
    "            rel = relevance_map[item_id]\n",
    "            # DCG formula: (2^rel - 1) / log2(i+2)\n",
    "            dcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Calculate ideal DCG (IDCG)\n",
    "    # Sort relevant items by their relevance scores in descending order\n",
    "    sorted_relevant = sorted(zip(relevant_items, relevant_scores), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    idcg = 0\n",
    "    for i, (item_id, rel) in enumerate(sorted_relevant[:k]):\n",
    "        # IDCG formula: (2^rel - 1) / log2(i+2)\n",
    "        idcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate NDCG\n",
    "    ndcg = dcg / idcg\n",
    "    \n",
    "    return ndcg\n",
    "\n",
    "def calculate_precision(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = num_relevant_recommended / len(recommended_items) if recommended_items else 0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def calculate_recall(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = num_relevant_recommended / len(relevant_items) if relevant_items else 0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def calculate_diversity_metrics(recommendations, item_popularity, total_items, tail_items=None):\n",
    "    \"\"\"\n",
    "    Calculate diversity metrics for a set of recommendations\n",
    "    \"\"\"\n",
    "    # Count occurrences of each item in recommendations\n",
    "    rec_counts = Counter(recommendations)\n",
    "    \n",
    "    # 1. Item Coverage\n",
    "    recommended_items = len(rec_counts)\n",
    "    item_coverage = recommended_items / total_items\n",
    "    \n",
    "    # 2. Gini Index\n",
    "    sorted_counts = sorted(rec_counts.values())\n",
    "    n = len(sorted_counts)\n",
    "    \n",
    "    if n == 0:\n",
    "        gini_index = 0\n",
    "    else:\n",
    "        cumulative_sum = 0\n",
    "        for i, count in enumerate(sorted_counts):\n",
    "            cumulative_sum += (i + 1) * count\n",
    "        \n",
    "        # Gini index formula\n",
    "        gini_index = (2 * cumulative_sum) / (n * sum(sorted_counts)) - (n + 1) / n\n",
    "    \n",
    "    # 3. Shannon Entropy\n",
    "    recommendations_count = sum(rec_counts.values())\n",
    "    probabilities = [count / recommendations_count for count in rec_counts.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    \n",
    "    # Normalize entropy\n",
    "    max_entropy = np.log2(min(total_items, recommendations_count))\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    \n",
    "    # 4. Tail Percentage\n",
    "    if tail_items is None:\n",
    "        # If tail_items not provided, use the bottom 20% by popularity\n",
    "        sorted_pop_indices = np.argsort(item_popularity)\n",
    "        num_tail_items = int(len(sorted_pop_indices) * 0.2)  # 20% least popular items\n",
    "        tail_items = set(sorted_pop_indices[:num_tail_items])\n",
    "    \n",
    "    tail_recommendations = sum(1 for item in recommendations if item in tail_items)\n",
    "    tail_percentage = tail_recommendations / len(recommendations) if recommendations else 0\n",
    "    \n",
    "    # Create results dictionary\n",
    "    metrics = {\n",
    "        'item_coverage': item_coverage,\n",
    "        'gini_index': gini_index,\n",
    "        'shannon_entropy': normalized_entropy,\n",
    "        'tail_percentage': tail_percentage\n",
    "    }\n",
    "    \n",
    "    return metrics, tail_items\n",
    "\n",
    "#################################\n",
    "# HELPER FUNCTIONS\n",
    "#################################\n",
    "\n",
    "def load_movielens_100k(path=\"ml-100k\"):\n",
    "    \"\"\"\n",
    "    Load the MovieLens 100K dataset\n",
    "    \"\"\"\n",
    "    # Load ratings\n",
    "    ratings_df = pd.read_csv(f\"{path}/u.data\", sep='\\t', \n",
    "                           names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Load movie information\n",
    "    movie_df = pd.read_csv(f\"{path}/u.item\", sep='|', encoding='latin-1',\n",
    "                          names=['item_id', 'title', 'release_date', 'video_release_date',\n",
    "                                 'IMDb_URL'] + [f'genre_{i}' for i in range(19)])\n",
    "    \n",
    "    return ratings_df, movie_df\n",
    "\n",
    "def create_user_item_matrix(ratings_df):\n",
    "    \"\"\"\n",
    "    Create a sparse user-item interaction matrix from ratings\n",
    "    \"\"\"\n",
    "    # Create mappings from original IDs to matrix indices\n",
    "    user_ids = ratings_df['user_id'].unique()\n",
    "    item_ids = ratings_df['item_id'].unique()\n",
    "    \n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(user_ids)}\n",
    "    item_mapping = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "    \n",
    "    # Map original IDs to matrix indices\n",
    "    rows = ratings_df['user_id'].map(user_mapping)\n",
    "    cols = ratings_df['item_id'].map(item_mapping)\n",
    "    \n",
    "    # Create binary matrix (1 if interaction exists, 0 otherwise)\n",
    "    data = np.ones(len(ratings_df))\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                 shape=(len(user_mapping), len(item_mapping)))\n",
    "    \n",
    "    return user_item_matrix, user_mapping, item_mapping\n",
    "\n",
    "#################################\n",
    "# COMPREHENSIVE EVALUATION\n",
    "#################################\n",
    "\n",
    "def comprehensive_evaluation_multiple_rerankers(k=10, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run a comprehensive evaluation measuring both accuracy and diversity for multiple rerankers\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k={k})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\nLoading MovieLens 100K dataset...\")\n",
    "    ratings_df, movie_df = load_movielens_100k()\n",
    "    \n",
    "    print(\"Splitting data for evaluation...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        ratings_df, \n",
    "        test_size=0.2, \n",
    "        stratify=ratings_df['user_id'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Creating user-item matrix...\")\n",
    "    user_item_matrix, user_mapping, item_mapping = create_user_item_matrix(train_df)\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    reverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "    reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    \n",
    "    # Create test set ground truth\n",
    "    test_relevant_items = defaultdict(list)\n",
    "    test_relevant_scores = defaultdict(list)\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Only include users and items that exist in our mappings\n",
    "        if user_id in user_mapping and item_id in item_mapping:\n",
    "            test_relevant_items[user_id].append(item_id)\n",
    "            test_relevant_scores[user_id].append(rating)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining ItemKNN model...\")\n",
    "    model = ItemKNNRecommender(k=50)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    # Initialize rerankers\n",
    "    print(\"\\nInitializing rerankers...\")\n",
    "    simple_reranker = SimpleReranker(model=model, alpha=0.7)\n",
    "    mmr_reranker = MMRReranker(model=model, lambda_param=0.7)\n",
    "    \n",
    "    # Setup dictionary for all rerankers' results\n",
    "    rerankers = {\n",
    "        \"Original ItemKNN\": None,\n",
    "        \"Simple Reranker\": simple_reranker,\n",
    "        \"MMR Reranker\": mmr_reranker\n",
    "    }\n",
    "    \n",
    "    # Results dictionary\n",
    "    all_results = {}\n",
    "    \n",
    "    # Select users for evaluation\n",
    "    if sample_size is not None and sample_size < len(test_relevant_items):\n",
    "        eval_users = random.sample(list(test_relevant_items.keys()), sample_size)\n",
    "    else:\n",
    "        eval_users = list(test_relevant_items.keys())\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(eval_users)} users...\")\n",
    "    \n",
    "    # Evaluate each reranker\n",
    "    for reranker_name, reranker in rerankers.items():\n",
    "        print(f\"\\nEvaluating {reranker_name}...\")\n",
    "        \n",
    "        # Initialize metrics collectors\n",
    "        ndcg_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        all_recs = []\n",
    "        \n",
    "        # Evaluate each user\n",
    "        for user_id in eval_users:\n",
    "            # Skip if user has no relevant items\n",
    "            if not test_relevant_items[user_id]:\n",
    "                continue\n",
    "            \n",
    "            user_idx = user_mapping[user_id]\n",
    "            \n",
    "            # Get recommendations\n",
    "            if reranker is None:  # Original ItemKNN\n",
    "                rec_idx = model.recommend(user_idx, n=k)\n",
    "            else:  # Use reranker\n",
    "                rec_idx = reranker.rerank(user_idx, n=k)\n",
    "                \n",
    "            rec = [reverse_item_mapping[idx] for idx in rec_idx]\n",
    "            all_recs.extend(rec_idx)\n",
    "            \n",
    "            # Calculate accuracy metrics\n",
    "            ndcg_scores.append(calculate_ndcg(\n",
    "                rec, test_relevant_items[user_id], test_relevant_scores[user_id]\n",
    "            ))\n",
    "            precision_scores.append(calculate_precision(\n",
    "                rec, test_relevant_items[user_id]\n",
    "            ))\n",
    "            recall_scores.append(calculate_recall(\n",
    "                rec, test_relevant_items[user_id]\n",
    "            ))\n",
    "        \n",
    "        # Calculate average accuracy metrics\n",
    "        accuracy_metrics = {\n",
    "            f'ndcg@{k}': np.mean(ndcg_scores),\n",
    "            f'precision@{k}': np.mean(precision_scores),\n",
    "            f'recall@{k}': np.mean(recall_scores)\n",
    "        }\n",
    "        \n",
    "        # Calculate diversity metrics\n",
    "        # First calculate item popularity\n",
    "        item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    item_popularity[item] += 1\n",
    "        \n",
    "        # Then calculate diversity metrics\n",
    "        diversity_metrics, _ = calculate_diversity_metrics(\n",
    "            recommendations=all_recs,\n",
    "            item_popularity=item_popularity,\n",
    "            total_items=model.n_items\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[reranker_name] = {\n",
    "            'accuracy': accuracy_metrics,\n",
    "            'diversity': diversity_metrics\n",
    "        }\n",
    "    \n",
    "    # Print comparative results\n",
    "    print(\"\\n\" + \"=\"*30 + \" ACCURACY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for reranker_name in rerankers.keys():\n",
    "        print(f\"{reranker_name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in [f'ndcg@{k}', f'precision@{k}', f'recall@{k}']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original ItemKNN\"]['accuracy'][metric]\n",
    "        for reranker_name in rerankers.keys():\n",
    "            value = all_results[reranker_name]['accuracy'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            \n",
    "            if reranker_name == \"Original ItemKNN\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \" DIVERSITY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for reranker_name in rerankers.keys():\n",
    "        print(f\"{reranker_name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in ['item_coverage', 'gini_index', 'shannon_entropy', 'tail_percentage']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original ItemKNN\"]['diversity'][metric]\n",
    "        for reranker_name in rerankers.keys():\n",
    "            value = all_results[reranker_name]['diversity'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            \n",
    "            if reranker_name == \"Original ItemKNN\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # Print interpretations\n",
    "    print(\"\\n\" + \"=\"*30 + \" METRIC INTERPRETATIONS \" + \"=\"*30)\n",
    "    print(\"Accuracy Metrics:\")\n",
    "    print(\"- NDCG: Higher is better, measures ranking quality\")\n",
    "    print(\"- Precision: Higher is better, measures relevant item ratio in recommendations\")\n",
    "    print(\"- Recall: Higher is better, measures coverage of all relevant items\")\n",
    "    \n",
    "    print(\"\\nDiversity Metrics:\")\n",
    "    print(\"- Item Coverage: Higher means more catalog items are recommended\")\n",
    "    print(\"- Gini Index: Lower means more equality in item recommendations\")\n",
    "    print(\"- Shannon Entropy: Higher means more diverse recommendations\")\n",
    "    print(\"- Tail Percentage: Higher means more niche items are recommended\")\n",
    "    \n",
    "    # Return all results\n",
    "    return all_results\n",
    "\n",
    "# Execute with multiple rerankers when running the script directly\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_evaluation_multiple_rerankers(k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
