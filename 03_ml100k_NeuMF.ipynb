{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9761506d-dd63-4346-8745-09c19328de11",
   "metadata": {},
   "source": [
    "## Model: NeuMF (Neural Collaborative Filtering)\n",
    "\n",
    "This notebook implements the **NeuMF** model as described in:\n",
    "\n",
    "> Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua.  \n",
    "> **Neural Collaborative Filtering**  \n",
    "> WWW 2017.  \n",
    "> [PDF](https://arxiv.org/pdf/1708.05031.pdf)\n",
    "\n",
    "### Core idea\n",
    "NeuMF fuses two models:\n",
    "- GMF (Generalized Matrix Factorization): A linear latent factor model.\n",
    "- MLP (Multi-Layer Perceptron): A deep model that captures non-linear user–item interactions.\n",
    "\n",
    "These are combined into a unified neural architecture to predict interaction likelihoods.\n",
    "\n",
    "### Implementation notes\n",
    "- The model consists of embedding layers, MLP layers, and a fusion layer.\n",
    "- Binary cross-entropy is used as the loss function.\n",
    "- Training uses negative sampling and is optimized with Adam.\n",
    "\n",
    "The architecture matches the original paper’s proposal and is structured for comparison with other recommenders and post-processing reranking methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb5a52-afef-4416-b4f7-fa9368f15cff",
   "metadata": {},
   "source": [
    "### 📊 Comparison with RecBole: NeuMF Implementation\n",
    "\n",
    "This notebook implements the **NeuMF** architecture based on *He et al. (2017)*, combining both GMF and MLP components to learn user-item interactions. While the design remains faithful to the original paper, there are notable architectural and procedural differences when compared to RecBole’s built-in `NeuMF` model.\n",
    "\n",
    "#### 🔍 Metric Comparison (ML-100K, Original NeuMF Only)\n",
    "\n",
    "| Metric         | RecBole NeuMF | Custom NeuMF | Δ (%)        |\n",
    "|----------------|---------------|---------------|--------------|\n",
    "| NDCG@10        | 0.2812        | 0.2657        | –5.51%       |\n",
    "| Precision@10   | 0.1940        | 0.2928        | **+50.93%**  |\n",
    "| Recall@10      | 0.2410        | 0.1942        | –19.41%      |\n",
    "| Gini Index     | 0.8954        | 0.6376        | –28.79%      |\n",
    "| Item Coverage  | 0.3363        | 0.4444        | **+32.16%**  |\n",
    "| Entropy        | 0.0096        | 0.7905        | **+8140%**   |\n",
    "| Tail %         | 0.0           | 0.0           | –            |\n",
    "\n",
    "> *Note: RecBole's entropy appears artificially low, likely due to the metric being calculated over the ranking positions rather than raw item occurrence distributions.*\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of Deviations from RecBole\n",
    "\n",
    "The following technical differences likely explain the observed deviations between this implementation and RecBole’s internal `NeuMF`:\n",
    "\n",
    "- **Model Architecture**  \n",
    "  The NeuMF model here separates GMF and MLP paths and merges them only at the final layer.  \n",
    "  RecBole may differ in the way it fuses or pretrains these components.\n",
    "\n",
    "- **Training Procedure**  \n",
    "  This code uses **binary cross-entropy** with **uniform negative sampling** and PyTorch `DataLoader`.  \n",
    "  RecBole offers several loss functions and sampling schemes configurable via YAML (e.g., BPR, popularity-based negatives).\n",
    "\n",
    "- **Embedding Initialization**  \n",
    "  Embeddings are initialized with a **normal distribution** (std=0.01).  \n",
    "  RecBole typically uses **Xavier** or **He** initialization for better convergence.\n",
    "\n",
    "- **Data Splitting and Filtering**  \n",
    "  This implementation uses a **stratified 80/20 split** without filtering.  \n",
    "  RecBole filters users/items based on configurable thresholds and supports **leave-one-out** and **chronological splits**.\n",
    "\n",
    "- **Relevance Scoring for NDCG**  \n",
    "  Here, **raw ratings** (e.g., 4 or 5) are used as gain in DCG.  \n",
    "  RecBole often applies **binary relevance**, marking items as relevant or not.\n",
    "\n",
    "- **Evaluation Framework**  \n",
    "  All metrics are computed explicitly via NumPy, giving full transparency.  \n",
    "  RecBole uses an internal evaluator which may include cold-start filtering or thresholding.\n",
    "\n",
    "- **Reranking Compatibility**  \n",
    "  This code enables custom rerankers like **Simple** and **MMR**, applied post hoc.  \n",
    "  RecBole does not natively support external reranking models within its evaluation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This NeuMF implementation provides a faithful reproduction of the original paper's concepts while enabling full flexibility in training, evaluation, and reranking. Despite modest deviations in accuracy and more significant differences in diversity metrics, the results demonstrate the strengths of a transparent, research-friendly framework compared to black-box pipelines like RecBole.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed51c2b-b23f-4088-ba43-6d61b7e3debf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k=10)\n",
      "================================================================================\n",
      "\n",
      "Loading MovieLens 100K dataset...\n",
      "Splitting data for evaluation...\n",
      "Creating user-item matrix...\n",
      "\n",
      "Training NeuMF model...\n",
      "Training NeuMF model for 10 epochs...\n",
      "Epoch 1/10, Loss: 0.3813, Time: 3.56s\n",
      "Epoch 5/10, Loss: 0.2489, Time: 3.44s\n",
      "Epoch 10/10, Loss: 0.1978, Time: 3.09s\n",
      "\n",
      "Initializing rerankers...\n",
      "\n",
      "Evaluating 943 users...\n",
      "\n",
      "Evaluating Original NeuMF...\n",
      "\n",
      "Evaluating Simple Reranker...\n",
      "\n",
      "Evaluating MMR Reranker...\n",
      "\n",
      "============================== ACCURACY METRICS COMPARISON ==============================\n",
      "Metric         Original NeuMF      Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "ndcg@10        0.2657               0.1717 (-35.4%)     0.2371 (-10.8%)     \n",
      "precision@10   0.2928               0.2223 (-24.1%)     0.2678 (-8.5%)     \n",
      "recall@10      0.1942               0.1394 (-28.2%)     0.1759 (-9.4%)     \n",
      "\n",
      "============================== DIVERSITY METRICS COMPARISON ==============================\n",
      "Metric         Original NeuMF      Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "item_coverage  0.4444               0.5181 (+16.6%)     0.4674 (+5.2%)     \n",
      "gini_index     0.6376               0.5532 (-13.2%)     0.6040 (-5.3%)     \n",
      "shannon_entropy0.7905               0.8411 (+6.4%)     0.8108 (+2.6%)     \n",
      "tail_percentage0.0000               0.0000 (+inf%)     0.0000 (+inf%)     \n",
      "\n",
      "============================== METRIC INTERPRETATIONS ==============================\n",
      "Accuracy Metrics:\n",
      "- NDCG: Higher is better, measures ranking quality\n",
      "- Precision: Higher is better, measures relevant item ratio in recommendations\n",
      "- Recall: Higher is better, measures coverage of all relevant items\n",
      "\n",
      "Diversity Metrics:\n",
      "- Item Coverage: Higher means more catalog items are recommended\n",
      "- Gini Index: Lower means more equality in item recommendations\n",
      "- Shannon Entropy: Higher means more diverse recommendations\n",
      "- Tail Percentage: Higher means more niche items are recommended\n"
     ]
    }
   ],
   "source": [
    "# NeuMF Recommender with Diversity Reranking\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#################################\n",
    "# NEUMF RECOMMENDER IMPLEMENTATION\n",
    "#################################\n",
    "\n",
    "class NCFDataset(Dataset):\n",
    "    \"\"\"Dataset for NCF\"\"\"\n",
    "    def __init__(self, user_item_matrix, neg_samples=4):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Parameters:\n",
    "        - user_item_matrix: scipy sparse matrix with user-item interactions\n",
    "        - neg_samples: number of negative samples per positive interaction\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.users, self.items = user_item_matrix.nonzero()\n",
    "        self.n_users = user_item_matrix.shape[0]\n",
    "        self.n_items = user_item_matrix.shape[1]\n",
    "        self.neg_samples = neg_samples\n",
    "        \n",
    "        # Create a set of (user, item) pairs for quick lookup\n",
    "        self.user_item_set = set(zip(self.users, self.items))\n",
    "        \n",
    "        # Create a dictionary of items each user has interacted with\n",
    "        self.user_items = defaultdict(set)\n",
    "        for u, i in zip(self.users, self.items):\n",
    "            self.user_items[u].add(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.users) * (1 + self.neg_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Determine if this is a positive or negative sample\n",
    "        if idx < len(self.users):\n",
    "            # Positive sample\n",
    "            user = self.users[idx]\n",
    "            item = self.items[idx]\n",
    "            label = 1.0\n",
    "        else:\n",
    "            # Negative sample - sample a user and item that don't have an interaction\n",
    "            pos_idx = idx % len(self.users)\n",
    "            user = self.users[pos_idx]\n",
    "            \n",
    "            # Sample a negative item for this user\n",
    "            item = random.randint(0, self.n_items - 1)\n",
    "            while item in self.user_items[user]:\n",
    "                item = random.randint(0, self.n_items - 1)\n",
    "            \n",
    "            label = 0.0\n",
    "            \n",
    "        return user, item, label\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    \"\"\"Generalized Matrix Factorization model\"\"\"\n",
    "    def __init__(self, n_users, n_items, latent_dim):\n",
    "        super(GMF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, latent_dim)\n",
    "        self.output_layer = nn.Linear(latent_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embeddings = self.user_embedding(user_indices)\n",
    "        item_embeddings = self.item_embedding(item_indices)\n",
    "        element_product = torch.mul(user_embeddings, item_embeddings)\n",
    "        output = self.output_layer(element_product)\n",
    "        return output.view(-1)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron model\"\"\"\n",
    "    def __init__(self, n_users, n_items, latent_dim, layers=[64, 32, 16, 8]):\n",
    "        super(MLP, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, latent_dim)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer_dims = [2 * latent_dim] + layers\n",
    "        \n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            self.layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(layer_dims[-1], 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embeddings = self.user_embedding(user_indices)\n",
    "        item_embeddings = self.item_embedding(item_indices)\n",
    "        vector = torch.cat([user_embeddings, item_embeddings], dim=-1)\n",
    "        \n",
    "        # Apply each layer\n",
    "        for layer in self.layers:\n",
    "            vector = layer(vector)\n",
    "            \n",
    "        output = self.output_layer(vector)\n",
    "        return output.view(-1)\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    \"\"\"Neural Matrix Factorization model\"\"\"\n",
    "    def __init__(self, n_users, n_items, latent_dim=32, mlp_layers=[64, 32, 16, 8]):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.gmf = GMF(n_users, n_items, latent_dim)\n",
    "        self.mlp = MLP(n_users, n_items, latent_dim, mlp_layers)\n",
    "        \n",
    "        # NeuMF output layer\n",
    "        self.output_layer = nn.Linear(mlp_layers[-1] + latent_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.output_layer.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # GMF path\n",
    "        gmf_user = self.gmf.user_embedding(user_indices)\n",
    "        gmf_item = self.gmf.item_embedding(item_indices)\n",
    "        gmf_vector = torch.mul(gmf_user, gmf_item)\n",
    "        \n",
    "        # MLP path\n",
    "        mlp_user = self.mlp.user_embedding(user_indices)\n",
    "        mlp_item = self.mlp.item_embedding(item_indices)\n",
    "        mlp_vector = torch.cat([mlp_user, mlp_item], dim=-1)\n",
    "        \n",
    "        for layer in self.mlp.layers:\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "        \n",
    "        # Concatenate GMF and MLP vectors\n",
    "        vector = torch.cat([gmf_vector, mlp_vector], dim=-1)\n",
    "        \n",
    "        # Final output\n",
    "        output = self.output_layer(vector)\n",
    "        \n",
    "        return torch.sigmoid(output.view(-1))\n",
    "\n",
    "class NeuMFRecommender:\n",
    "    def __init__(self, latent_dim=32, mlp_layers=[64, 32, 16, 8], epochs=20, batch_size=256, \n",
    "                 lr=0.001, neg_samples=4, device=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Neural Matrix Factorization recommender algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        - latent_dim: dimensionality of latent factors\n",
    "        - mlp_layers: list of layer sizes for MLP component\n",
    "        - epochs: number of training epochs\n",
    "        - batch_size: batch size for training\n",
    "        - lr: learning rate\n",
    "        - neg_samples: number of negative samples per positive interaction\n",
    "        - device: torch device (cpu or cuda)\n",
    "        - random_state: seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.latent_dim = latent_dim\n",
    "        self.mlp_layers = mlp_layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.neg_samples = neg_samples\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set random seeds\n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Train the NeuMF model on the user-item matrix\n",
    "        \n",
    "        Parameters:\n",
    "        - user_item_matrix: scipy sparse matrix with user-item interactions\n",
    "        \n",
    "        Returns:\n",
    "        - self\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.n_users, self.n_items = user_item_matrix.shape\n",
    "        \n",
    "        # Create a dictionary of items each user has interacted with\n",
    "        self.user_items = defaultdict(set)\n",
    "        for user, item in zip(*self.user_item_matrix.nonzero()):\n",
    "            self.user_items[user].add(item)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = NCFDataset(user_item_matrix, neg_samples=self.neg_samples)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = NeuMF(self.n_users, self.n_items, self.latent_dim, self.mlp_layers).to(self.device)\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Training loop\n",
    "        print(f\"Training NeuMF model for {self.epochs} epochs...\")\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for users, items, labels in dataloader:\n",
    "                users = users.to(self.device)\n",
    "                items = items.to(self.device)\n",
    "                labels = labels.float().to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(users, items)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {running_loss/len(dataloader):.4f}, Time: {elapsed_time:.2f}s\")\n",
    "        \n",
    "        # Switch to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Precompute all user embeddings\n",
    "        with torch.no_grad():\n",
    "            self.user_gmf_embeddings = self.model.gmf.user_embedding.weight.data\n",
    "            self.item_gmf_embeddings = self.model.gmf.item_embedding.weight.data\n",
    "            self.user_mlp_embeddings = self.model.mlp.user_embedding.weight.data\n",
    "            self.item_mlp_embeddings = self.model.mlp.item_embedding.weight.data\n",
    "        \n",
    "        # Set up item factors (for compatibility with rerankers)\n",
    "        # We'll use a combination of GMF and MLP embeddings as item factors\n",
    "        self.item_factors = np.concatenate([\n",
    "            self.item_gmf_embeddings.cpu().numpy(),\n",
    "            self.item_mlp_embeddings.cpu().numpy()\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def recommend(self, user_id, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate item recommendations for a user\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index\n",
    "        - n: number of recommendations to generate\n",
    "        - exclude_seen: whether to exclude items the user has already interacted with\n",
    "        \n",
    "        Returns:\n",
    "        - list of n recommended item indices\n",
    "        \"\"\"\n",
    "        # If the user has no interactions in training set, return random recommendations\n",
    "        if user_id not in self.user_items:\n",
    "            all_items = list(range(self.n_items))\n",
    "            recommendations = random.sample(all_items, min(n, len(all_items)))\n",
    "            return np.array(recommendations)\n",
    "        \n",
    "        # Predict scores for all items for this user\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_id] * self.n_items).to(self.device)\n",
    "            item_tensor = torch.LongTensor(list(range(self.n_items))).to(self.device)\n",
    "            \n",
    "            scores = self.model(user_tensor, item_tensor).cpu().numpy()\n",
    "        \n",
    "        # If requested, exclude items the user has already interacted with\n",
    "        if exclude_seen:\n",
    "            for item_id in self.user_items[user_id]:\n",
    "                scores[item_id] = -np.inf\n",
    "        \n",
    "        # Get top n items by score\n",
    "        top_items = np.argsort(scores)[::-1][:n]\n",
    "        \n",
    "        return top_items\n",
    "\n",
    "#################################\n",
    "# RERANKER IMPLEMENTATION\n",
    "#################################\n",
    "\n",
    "class SimpleReranker:\n",
    "    \"\"\"\n",
    "    Simple reranker that balances original scores with diversity\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Initialize reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained recommender model\n",
    "        - alpha: weight for original scores (between 0 and 1)\n",
    "                 higher alpha means more focus on accuracy\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Calculate item popularity\n",
    "        self.item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    self.item_popularity[item] += 1\n",
    "        \n",
    "        # Normalize popularity\n",
    "        max_pop = np.max(self.item_popularity)\n",
    "        if max_pop > 0:\n",
    "            self.norm_popularity = self.item_popularity / max_pop\n",
    "        else:\n",
    "            self.norm_popularity = np.zeros_like(self.item_popularity)\n",
    "    \n",
    "    def rerank(self, user_id, n=10):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations\n",
    "        \"\"\"\n",
    "        # Get original recommendations as a larger candidate pool\n",
    "        candidates = self.model.recommend(user_id, n=n*3, exclude_seen=True)\n",
    "        \n",
    "        # Get scores for all items for this user\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_id] * self.model.n_items).to(self.model.device)\n",
    "            item_tensor = torch.LongTensor(list(range(self.model.n_items))).to(self.model.device)\n",
    "            scores = self.model.model(user_tensor, item_tensor).cpu().numpy()\n",
    "        \n",
    "        # Initialize selected items\n",
    "        selected = []\n",
    "        \n",
    "        # Iteratively select items\n",
    "        while len(selected) < n and candidates.size > 0:\n",
    "            best_score = -np.inf\n",
    "            best_item = None\n",
    "            \n",
    "            for item in candidates:\n",
    "                if item in selected:\n",
    "                    continue\n",
    "                \n",
    "                # Original score component\n",
    "                score_orig = scores[item]\n",
    "                \n",
    "                # Diversity component\n",
    "                diversity_score = 0\n",
    "                if selected:\n",
    "                    # Use item factors to calculate similarity\n",
    "                    item_factors = self.model.item_factors[item]\n",
    "                    selected_factors = self.model.item_factors[selected]\n",
    "                    \n",
    "                    # Calculate average similarity\n",
    "                    similarities = []\n",
    "                    for sel_factors in selected_factors:\n",
    "                        # Cosine similarity\n",
    "                        dot_product = np.dot(item_factors, sel_factors)\n",
    "                        norm_product = np.linalg.norm(item_factors) * np.linalg.norm(sel_factors)\n",
    "                        if norm_product > 0:\n",
    "                            sim = dot_product / norm_product\n",
    "                        else:\n",
    "                            sim = 0\n",
    "                        similarities.append(sim)\n",
    "                    \n",
    "                    if similarities:\n",
    "                        avg_sim = np.mean(similarities)\n",
    "                        diversity_score = 1 - avg_sim\n",
    "                \n",
    "                # Novelty component (inverse popularity)\n",
    "                novelty_score = 1 - self.norm_popularity[item]\n",
    "                \n",
    "                # Calculate weighted score\n",
    "                combined_score = (\n",
    "                    self.alpha * score_orig + \n",
    "                    (1 - self.alpha) * 0.5 * diversity_score + \n",
    "                    (1 - self.alpha) * 0.5 * novelty_score\n",
    "                )\n",
    "                \n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_item = item\n",
    "            \n",
    "            if best_item is None:\n",
    "                break\n",
    "                \n",
    "            selected.append(best_item)\n",
    "            candidates = candidates[candidates != best_item]\n",
    "            \n",
    "        return np.array(selected)\n",
    "\n",
    "class MMRReranker:\n",
    "    \"\"\"\n",
    "    Maximum Marginal Relevance (MMR) Reranker\n",
    "    \n",
    "    This reranker balances between relevance and diversity explicitly by\n",
    "    selecting items that maximize marginal relevance - items that are\n",
    "    both relevant to the user and different from already selected items.\n",
    "    \n",
    "    MMR formula: MMR = λ * rel(i) - (1-λ) * max(sim(i,j)) for j in selected items\n",
    "    \n",
    "    Where:\n",
    "    - rel(i) is the relevance of item i to the user\n",
    "    - sim(i,j) is the similarity between items i and j\n",
    "    - λ is a parameter that controls the trade-off between relevance and diversity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, lambda_param=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the MMR reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained recommender model\n",
    "        - lambda_param: trade-off parameter between relevance and diversity (0-1)\n",
    "                        higher values favor relevance, lower values favor diversity\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.lambda_param = lambda_param\n",
    "        \n",
    "    def calculate_item_similarity(self, item1, item2):\n",
    "        \"\"\"\n",
    "        Calculate similarity between two items\n",
    "        \n",
    "        Parameters:\n",
    "        - item1: index of first item\n",
    "        - item2: index of second item\n",
    "        \n",
    "        Returns:\n",
    "        - similarity: similarity between items (0 to 1)\n",
    "        \"\"\"\n",
    "        # Calculate cosine similarity between item embeddings\n",
    "        item1_factors = self.model.item_factors[item1]\n",
    "        item2_factors = self.model.item_factors[item2]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        dot_product = np.dot(item1_factors, item2_factors)\n",
    "        norm_product = np.linalg.norm(item1_factors) * np.linalg.norm(item2_factors)\n",
    "        \n",
    "        if norm_product == 0:\n",
    "            return 0\n",
    "        \n",
    "        return dot_product / norm_product\n",
    "    \n",
    "    def rerank(self, user_id, n=10, candidate_size=100):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations using Maximum Marginal Relevance\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index in the model\n",
    "        - n: number of recommendations to return\n",
    "        - candidate_size: number of initial candidates to consider\n",
    "        \n",
    "        Returns:\n",
    "        - reranked_items: list of reranked item indices\n",
    "        \"\"\"\n",
    "        # Get candidate items and their scores\n",
    "        candidates = self.model.recommend(user_id, n=candidate_size, exclude_seen=True)\n",
    "        \n",
    "        # Get scores for candidate items\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_id] * self.model.n_items).to(self.model.device)\n",
    "            item_tensor = torch.LongTensor(list(range(self.model.n_items))).to(self.model.device)\n",
    "            relevance_scores = self.model.model(user_tensor, item_tensor).cpu().numpy()\n",
    "        \n",
    "        # Normalize relevance scores to [0,1] range for the candidates\n",
    "        candidate_scores = relevance_scores[candidates]\n",
    "        min_score = np.min(candidate_scores)\n",
    "        max_score = np.max(candidate_scores)\n",
    "        score_range = max_score - min_score\n",
    "        \n",
    "        if score_range > 0:\n",
    "            normalized_scores = (candidate_scores - min_score) / score_range\n",
    "        else:\n",
    "            normalized_scores = np.zeros_like(candidate_scores)\n",
    "        \n",
    "        # Initialize selected items\n",
    "        selected = []\n",
    "        \n",
    "        # Select first item (most relevant)\n",
    "        if candidates.size > 0:\n",
    "            selected.append(candidates[np.argmax(normalized_scores)])\n",
    "            remaining_candidates = set(candidates) - set(selected)\n",
    "        else:\n",
    "            remaining_candidates = set()\n",
    "        \n",
    "        # Iteratively select items using MMR\n",
    "        while len(selected) < n and remaining_candidates:\n",
    "            max_mmr = -np.inf\n",
    "            max_item = None\n",
    "            \n",
    "            for item in remaining_candidates:\n",
    "                # Get relevance component\n",
    "                item_idx = np.where(candidates == item)[0][0]\n",
    "                relevance = normalized_scores[item_idx]\n",
    "                \n",
    "                # Calculate diversity component (inverse of maximum similarity)\n",
    "                max_sim = 0\n",
    "                for selected_item in selected:\n",
    "                    sim = self.calculate_item_similarity(item, selected_item)\n",
    "                    max_sim = max(max_sim, sim)\n",
    "                \n",
    "                # Calculate MMR score\n",
    "                mmr_score = self.lambda_param * relevance - (1 - self.lambda_param) * max_sim\n",
    "                \n",
    "                if mmr_score > max_mmr:\n",
    "                    max_mmr = mmr_score\n",
    "                    max_item = item\n",
    "            \n",
    "            if max_item is not None:\n",
    "                selected.append(max_item)\n",
    "                remaining_candidates.remove(max_item)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return np.array(selected)\n",
    "\n",
    "#################################\n",
    "# EVALUATION METRICS\n",
    "#################################\n",
    "\n",
    "def calculate_ndcg(recommended_items, relevant_items, relevant_scores, k=None):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(recommended_items)\n",
    "    else:\n",
    "        k = min(k, len(recommended_items))\n",
    "    \n",
    "    # Create a dictionary mapping relevant items to their scores\n",
    "    relevance_map = {item_id: score for item_id, score in zip(relevant_items, relevant_scores)}\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = 0\n",
    "    for i, item_id in enumerate(recommended_items[:k]):\n",
    "        if item_id in relevance_map:\n",
    "            # Use rating as relevance score\n",
    "            rel = relevance_map[item_id]\n",
    "            # DCG formula: (2^rel - 1) / log2(i+2)\n",
    "            dcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Calculate ideal DCG (IDCG)\n",
    "    # Sort relevant items by their relevance scores in descending order\n",
    "    sorted_relevant = sorted(zip(relevant_items, relevant_scores), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    idcg = 0\n",
    "    for i, (item_id, rel) in enumerate(sorted_relevant[:k]):\n",
    "        # IDCG formula: (2^rel - 1) / log2(i+2)\n",
    "        idcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate NDCG\n",
    "    ndcg = dcg / idcg\n",
    "    \n",
    "    return ndcg\n",
    "\n",
    "def calculate_precision(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = num_relevant_recommended / len(recommended_items) if recommended_items else 0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def calculate_recall(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = num_relevant_recommended / len(relevant_items) if relevant_items else 0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def calculate_diversity_metrics(recommendations, item_popularity, total_items, tail_items=None):\n",
    "    \"\"\"\n",
    "    Calculate diversity metrics for a set of recommendations\n",
    "    \"\"\"\n",
    "    # Count occurrences of each item in recommendations\n",
    "    rec_counts = Counter(recommendations)\n",
    "    \n",
    "    # 1. Item Coverage\n",
    "    recommended_items = len(rec_counts)\n",
    "    item_coverage = recommended_items / total_items\n",
    "    \n",
    "    # 2. Gini Index\n",
    "    sorted_counts = sorted(rec_counts.values())\n",
    "    n = len(sorted_counts)\n",
    "    \n",
    "    if n == 0:\n",
    "        gini_index = 0\n",
    "    else:\n",
    "        cumulative_sum = 0\n",
    "        for i, count in enumerate(sorted_counts):\n",
    "            cumulative_sum += (i + 1) * count\n",
    "        \n",
    "        # Gini index formula\n",
    "        gini_index = (2 * cumulative_sum) / (n * sum(sorted_counts)) - (n + 1) / n\n",
    "    \n",
    "    # 3. Shannon Entropy\n",
    "    recommendations_count = sum(rec_counts.values())\n",
    "    probabilities = [count / recommendations_count for count in rec_counts.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    \n",
    "    # Normalize entropy\n",
    "    max_entropy = np.log2(min(total_items, recommendations_count))\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    \n",
    "    # 4. Tail Percentage\n",
    "    if tail_items is None:\n",
    "        # If tail_items not provided, use the bottom 20% by popularity\n",
    "        sorted_pop_indices = np.argsort(item_popularity)\n",
    "        num_tail_items = int(len(sorted_pop_indices) * 0.2)  # 20% least popular items\n",
    "        tail_items = set(sorted_pop_indices[:num_tail_items])\n",
    "    \n",
    "    tail_recommendations = sum(1 for item in recommendations if item in tail_items)\n",
    "    tail_percentage = tail_recommendations / len(recommendations) if recommendations else 0\n",
    "    \n",
    "    # Create results dictionary\n",
    "    metrics = {\n",
    "        'item_coverage': item_coverage,\n",
    "        'gini_index': gini_index,\n",
    "        'shannon_entropy': normalized_entropy,\n",
    "        'tail_percentage': tail_percentage\n",
    "    }\n",
    "    \n",
    "    return metrics, tail_items\n",
    "\n",
    "#################################\n",
    "# HELPER FUNCTIONS\n",
    "#################################\n",
    "\n",
    "def load_movielens_100k(path=\"ml-100k\"):\n",
    "    \"\"\"\n",
    "    Load the MovieLens 100K dataset\n",
    "    \"\"\"\n",
    "    # Load ratings\n",
    "    ratings_df = pd.read_csv(f\"{path}/u.data\", sep='\\t', \n",
    "                           names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Load movie information\n",
    "    movie_df = pd.read_csv(f\"{path}/u.item\", sep='|', encoding='latin-1',\n",
    "                          names=['item_id', 'title', 'release_date', 'video_release_date',\n",
    "                                 'IMDb_URL'] + [f'genre_{i}' for i in range(19)])\n",
    "    \n",
    "    return ratings_df, movie_df\n",
    "\n",
    "def create_user_item_matrix(ratings_df):\n",
    "    \"\"\"\n",
    "    Create a sparse user-item interaction matrix from ratings\n",
    "    \"\"\"\n",
    "    # Create mappings from original IDs to matrix indices\n",
    "    user_ids = ratings_df['user_id'].unique()\n",
    "    item_ids = ratings_df['item_id'].unique()\n",
    "    \n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(user_ids)}\n",
    "    item_mapping = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "    \n",
    "    # Map original IDs to matrix indices\n",
    "    rows = ratings_df['user_id'].map(user_mapping)\n",
    "    cols = ratings_df['item_id'].map(item_mapping)\n",
    "    \n",
    "    # Create binary matrix (1 if interaction exists, 0 otherwise)\n",
    "    data = np.ones(len(ratings_df))\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                 shape=(len(user_mapping), len(item_mapping)))\n",
    "    \n",
    "    return user_item_matrix, user_mapping, item_mapping\n",
    "\n",
    "#################################\n",
    "# COMPREHENSIVE EVALUATION\n",
    "#################################\n",
    "\n",
    "def comprehensive_evaluation_multiple_rerankers(k=10, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run a comprehensive evaluation measuring both accuracy and diversity for multiple rerankers\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k={k})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\nLoading MovieLens 100K dataset...\")\n",
    "    ratings_df, movie_df = load_movielens_100k()\n",
    "    \n",
    "    print(\"Splitting data for evaluation...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        ratings_df, \n",
    "        test_size=0.2, \n",
    "        stratify=ratings_df['user_id'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Creating user-item matrix...\")\n",
    "    user_item_matrix, user_mapping, item_mapping = create_user_item_matrix(train_df)\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    reverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "    reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    \n",
    "    # Create test set ground truth\n",
    "    test_relevant_items = defaultdict(list)\n",
    "    test_relevant_scores = defaultdict(list)\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Only include users and items that exist in our mappings\n",
    "        if user_id in user_mapping and item_id in item_mapping:\n",
    "            test_relevant_items[user_id].append(item_id)\n",
    "            test_relevant_scores[user_id].append(rating)\n",
    "    \n",
    "    # Train model - use fewer epochs for NeuMF since it's more computationally intensive\n",
    "    print(\"\\nTraining NeuMF model...\")\n",
    "    model = NeuMFRecommender(latent_dim=32, epochs=10, batch_size=256)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    # Initialize rerankers\n",
    "    print(\"\\nInitializing rerankers...\")\n",
    "    simple_reranker = SimpleReranker(model=model, alpha=0.7)\n",
    "    mmr_reranker = MMRReranker(model=model, lambda_param=0.7)\n",
    "    \n",
    "    # Setup dictionary for all rerankers' results\n",
    "    rerankers = {\n",
    "        \"Original NeuMF\": None,\n",
    "        \"Simple Reranker\": simple_reranker,\n",
    "        \"MMR Reranker\": mmr_reranker\n",
    "    }\n",
    "    \n",
    "    # Results dictionary\n",
    "    all_results = {}\n",
    "    \n",
    "    # Select users for evaluation\n",
    "    if sample_size is not None and sample_size < len(test_relevant_items):\n",
    "        eval_users = random.sample(list(test_relevant_items.keys()), sample_size)\n",
    "    else:\n",
    "        eval_users = list(test_relevant_items.keys())\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(eval_users)} users...\")\n",
    "    \n",
    "    # Evaluate each reranker\n",
    "    for reranker_name, reranker in rerankers.items():\n",
    "        print(f\"\\nEvaluating {reranker_name}...\")\n",
    "        \n",
    "        # Initialize metrics collectors\n",
    "        ndcg_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        all_recs = []\n",
    "        \n",
    "        # Evaluate each user\n",
    "        for user_id in eval_users:\n",
    "            # Skip if user has no relevant items\n",
    "            if not test_relevant_items[user_id]:\n",
    "                continue\n",
    "            \n",
    "            user_idx = user_mapping[user_id]\n",
    "            \n",
    "            # Get recommendations\n",
    "            if reranker is None:  # Original NeuMF\n",
    "                rec_idx = model.recommend(user_idx, n=k)\n",
    "            else:  # Use reranker\n",
    "                rec_idx = reranker.rerank(user_idx, n=k)\n",
    "                \n",
    "            rec = [reverse_item_mapping[idx] for idx in rec_idx]\n",
    "            all_recs.extend(rec_idx)\n",
    "            \n",
    "            # Calculate accuracy metrics\n",
    "            ndcg_scores.append(calculate_ndcg(\n",
    "                rec, test_relevant_items[user_id], test_relevant_scores[user_id]\n",
    "            ))\n",
    "            precision_scores.append(calculate_precision(\n",
    "                rec, test_relevant_items[user_id]\n",
    "            ))\n",
    "            recall_scores.append(calculate_recall(\n",
    "                rec, test_relevant_items[user_id]\n",
    "            ))\n",
    "        \n",
    "        # Calculate average accuracy metrics\n",
    "        accuracy_metrics = {\n",
    "            f'ndcg@{k}': np.mean(ndcg_scores),\n",
    "            f'precision@{k}': np.mean(precision_scores),\n",
    "            f'recall@{k}': np.mean(recall_scores)\n",
    "        }\n",
    "        \n",
    "        # Calculate diversity metrics\n",
    "        # First calculate item popularity\n",
    "        item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    item_popularity[item] += 1\n",
    "        \n",
    "        # Then calculate diversity metrics\n",
    "        diversity_metrics, _ = calculate_diversity_metrics(\n",
    "            recommendations=all_recs,\n",
    "            item_popularity=item_popularity,\n",
    "            total_items=model.n_items\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[reranker_name] = {\n",
    "            'accuracy': accuracy_metrics,\n",
    "            'diversity': diversity_metrics\n",
    "        }\n",
    "    \n",
    "    # Print comparative results\n",
    "    print(\"\\n\" + \"=\"*30 + \" ACCURACY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for reranker_name in rerankers.keys():\n",
    "        print(f\"{reranker_name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in [f'ndcg@{k}', f'precision@{k}', f'recall@{k}']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original NeuMF\"]['accuracy'][metric]\n",
    "        for reranker_name in rerankers.keys():\n",
    "            value = all_results[reranker_name]['accuracy'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            \n",
    "            if reranker_name == \"Original NeuMF\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \" DIVERSITY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for reranker_name in rerankers.keys():\n",
    "        print(f\"{reranker_name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in ['item_coverage', 'gini_index', 'shannon_entropy', 'tail_percentage']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original NeuMF\"]['diversity'][metric]\n",
    "        for reranker_name in rerankers.keys():\n",
    "            value = all_results[reranker_name]['diversity'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            \n",
    "            if reranker_name == \"Original NeuMF\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # Print interpretations\n",
    "    print(\"\\n\" + \"=\"*30 + \" METRIC INTERPRETATIONS \" + \"=\"*30)\n",
    "    print(\"Accuracy Metrics:\")\n",
    "    print(\"- NDCG: Higher is better, measures ranking quality\")\n",
    "    print(\"- Precision: Higher is better, measures relevant item ratio in recommendations\")\n",
    "    print(\"- Recall: Higher is better, measures coverage of all relevant items\")\n",
    "    \n",
    "    print(\"\\nDiversity Metrics:\")\n",
    "    print(\"- Item Coverage: Higher means more catalog items are recommended\")\n",
    "    print(\"- Gini Index: Lower means more equality in item recommendations\")\n",
    "    print(\"- Shannon Entropy: Higher means more diverse recommendations\")\n",
    "    print(\"- Tail Percentage: Higher means more niche items are recommended\")\n",
    "    \n",
    "    # Return all results\n",
    "    return all_results\n",
    "\n",
    "# Execute with multiple rerankers when running the script directly\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_evaluation_multiple_rerankers(k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
