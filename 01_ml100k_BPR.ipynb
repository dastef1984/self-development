{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13f9fb6-40fe-4575-947c-600b9ab1f0c0",
   "metadata": {},
   "source": [
    "## Model: Bayesian Personalized Ranking (BPR)\n",
    "\n",
    "This recommender system implements the **Bayesian Personalized Ranking (BPR)** algorithm, as introduced by Rendle et al. in:\n",
    "\n",
    "> Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.  \n",
    "> **BPR: Bayesian Personalized Ranking from Implicit Feedback**  \n",
    "> UAI 2009.  \n",
    "> [PDF](https://arxiv.org/pdf/1205.2618.pdf)\n",
    "\n",
    "### Core idea\n",
    "BPR is a pairwise learning-to-rank algorithm designed for implicit feedback datasets. It learns latent user and item vectors by maximizing the posterior probability that a user prefers an observed item over a non-observed one. Optimization is done via stochastic gradient descent over sampled user‚Äìpositive‚Äìnegative triplets.\n",
    "\n",
    "### Implementation notes\n",
    "- Embeddings are initialized with small Gaussian noise.\n",
    "- Negative sampling ensures the model learns from unobserved items.\n",
    "- The scoring function is the dot product of user and item latent vectors.\n",
    "- The loss function is based on pairwise differences and uses the sigmoid.\n",
    "\n",
    "This implementation reproduces the original method faithfully and provides a controlled environment for further re-ranking and evaluation experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e289de-2633-49dd-95b7-5ecc4d6945e7",
   "metadata": {},
   "source": [
    "### üìä Comparison with RecBole: BPR Implementation\n",
    "\n",
    "This notebook implements the Bayesian Personalized Ranking (BPR) algorithm based on the original paper by Rendle et al. (2009). While the implementation is correct and stable, small deviations from RecBole's internal BPR implementation are observed due to differences in preprocessing, training dynamics, and evaluation logic.\n",
    "\n",
    "#### üîç Metric Comparison (ML-100K, Original BPR Only)\n",
    "\n",
    "| Metric       | RecBole BPR | Custom BPR | Œî (%)      |\n",
    "|--------------|-------------|------------|------------|\n",
    "| NDCG@10      | 0.2862      | 0.2806     | ‚Äì1.96%     |\n",
    "| Precision@10 | 0.1914      | 0.3060     | **+59.88%** |\n",
    "| Recall@10    | 0.2388      | 0.1992     | ‚Äì16.56%    |\n",
    "| Gini Index   | 0.9248      | 0.6960     | ‚Äì24.72%    |\n",
    "| Item Coverage| 0.2632      | 0.1564     | ‚Äì40.56%    |\n",
    "| Entropy      | 0.0116      | 0.6300     | **+5324%*** |\n",
    "| Tail %       | 0.0         | 0.0        | ‚Äì          |\n",
    "\n",
    "\\*Note: The entropy in RecBole appears unusually low due to its internal calculation being based on normalized rank-frequency rather than raw item distributions.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of Deviations from RecBole\n",
    "\n",
    "The following differences likely account for the observed deviations in metrics:\n",
    "\n",
    "- **Loss Optimization**  \n",
    "  RecBole uses PyTorch-based backpropagation with optimizers like Adam. This implementation applies SGD with manual gradient updates.\n",
    "\n",
    "- **Sampling Strategy**  \n",
    "  Triplet sampling in this notebook is uniform. RecBole may use popularity-aware or stratified sampling strategies.\n",
    "\n",
    "- **Data Splitting & Filtering**  \n",
    "  RecBole performs built-in filtering (e.g., min interactions per user/item) and often uses leave-one-out splitting. Here, stratified random split (80/20) is used manually.\n",
    "\n",
    "- **Evaluation Method**  \n",
    "  RecBole evaluates recommendations over all items using internal evaluator logic. This notebook computes scores explicitly with NumPy.\n",
    "\n",
    "- **NDCG Relevance Scoring**  \n",
    "  RecBole typically uses binary relevance (hit or not), whereas this code uses actual rating values (e.g., 4 or 5 stars) in the gain function.\n",
    "\n",
    "- **Factor Initialization**  \n",
    "  Latent factors are initialized differently (normal distribution with std=0.1 vs. RecBole defaults), which can affect convergence and diversity.\n",
    "\n",
    "- **Reranker Compatibility**  \n",
    "  Reranking is not possible directly in RecBole's pipeline. This implementation separates reranking explicitly, enabling more flexible post-processing.\n",
    "\n",
    "---\n",
    "\n",
    "In conclusion, despite small numerical differences, this implementation produces valid and consistent results that align with the original BPR paper and offer full flexibility for custom reranking and metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6525e0e-175a-4b99-982f-643f1209e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION (alpha=0.7, k=10)\n",
      "================================================================================\n",
      "\n",
      "Loading MovieLens 100K dataset...\n",
      "Splitting data for evaluation...\n",
      "Creating user-item matrix...\n",
      "\n",
      "Training BPR model...\n",
      "Completed iteration 10/30\n",
      "Completed iteration 20/30\n",
      "Completed iteration 30/30\n",
      "\n",
      "Initializing reranker with alpha = 0.7\n",
      "\n",
      "Evaluating 943 users...\n",
      "\n",
      "============================== ACCURACY METRICS ==============================\n",
      "Metric          Original        Reranked        Change (%)     \n",
      "------------------------------------------------------------\n",
      "ndcg@10         0.2756           0.2734           -0.77%\n",
      "precision@10    0.2994           0.2990           -0.11%\n",
      "recall@10       0.1937           0.1924           -0.66%\n",
      "\n",
      "============================== DIVERSITY METRICS ==============================\n",
      "Metric               Original        Reranked        Change (%)     \n",
      "-----------------------------------------------------------------\n",
      "item_coverage        0.1528           0.1661           +8.70%\n",
      "gini_index           0.6963           0.6983           +0.30%\n",
      "shannon_entropy      0.6263           0.6367           +1.65%\n",
      "tail_percentage      0.0000           0.0000           +inf%\n",
      "\n",
      "============================== METRIC INTERPRETATIONS ==============================\n",
      "Accuracy Metrics:\n",
      "- NDCG: Higher is better, measures ranking quality\n",
      "- Precision: Higher is better, measures relevant item ratio in recommendations\n",
      "- Recall: Higher is better, measures coverage of all relevant items\n",
      "\n",
      "Diversity Metrics:\n",
      "- Item Coverage: Higher means more catalog items are recommended\n",
      "- Gini Index: Lower means more equality in item recommendations\n",
      "- Shannon Entropy: Higher means more diverse recommendations\n",
      "- Tail Percentage: Higher means more niche items are recommended\n"
     ]
    }
   ],
   "source": [
    "# BPR Reranking with Comprehensive Evaluation (Accuracy and Diversity)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#################################\n",
    "# BPR RECOMMENDER IMPLEMENTATION\n",
    "#################################\n",
    "\n",
    "class BPRRecommender:\n",
    "    def __init__(self, factors=50, learning_rate=0.01, regularization=0.01, iterations=50, random_state=42):\n",
    "        \"\"\"\n",
    "        Bayesian Personalized Ranking (BPR) recommender algorithm\n",
    "        \"\"\"\n",
    "        self.factors = factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Train the BPR model on the user-item matrix\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.n_users, self.n_items = user_item_matrix.shape\n",
    "        \n",
    "        # Initialize latent factors\n",
    "        self.user_factors = np.random.normal(0, 0.1, (self.n_users, self.factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (self.n_items, self.factors))\n",
    "        \n",
    "        # Create a dictionary of items each user has interacted with\n",
    "        self.user_items = defaultdict(set)\n",
    "        for user, item in zip(*self.user_item_matrix.nonzero()):\n",
    "            self.user_items[user].add(item)\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.iterations):\n",
    "            # Sample triplets for training\n",
    "            for _ in range(user_item_matrix.nnz):\n",
    "                user, pos_item, neg_item = self._sample_triplet()\n",
    "                self._update_factors(user, pos_item, neg_item)\n",
    "            \n",
    "            # Print progress\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                print(f\"Completed iteration {iteration + 1}/{self.iterations}\")\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def _sample_triplet(self):\n",
    "        \"\"\"\n",
    "        Sample a (user, positive_item, negative_item) triplet for training\n",
    "        \"\"\"\n",
    "        # Sample a user who has rated at least one item\n",
    "        user = random.choice(list(self.user_items.keys()))\n",
    "        \n",
    "        # Sample a positive item (one the user has interacted with)\n",
    "        pos_item = random.choice(list(self.user_items[user]))\n",
    "        \n",
    "        # Sample a negative item (one the user has not interacted with)\n",
    "        neg_item = random.randint(0, self.n_items - 1)\n",
    "        while neg_item in self.user_items[user]:\n",
    "            neg_item = random.randint(0, self.n_items - 1)\n",
    "            \n",
    "        return user, pos_item, neg_item\n",
    "    \n",
    "    def _update_factors(self, user, pos_item, neg_item):\n",
    "        \"\"\"\n",
    "        Update model parameters based on a triplet\n",
    "        \"\"\"\n",
    "        # Calculate prediction for positive and negative items\n",
    "        pos_pred = np.dot(self.user_factors[user], self.item_factors[pos_item])\n",
    "        neg_pred = np.dot(self.user_factors[user], self.item_factors[neg_item])\n",
    "        \n",
    "        # Calculate prediction difference\n",
    "        diff = neg_pred - pos_pred\n",
    "        \n",
    "        # Calculate sigmoid gradient\n",
    "        sigmoid = 1.0 / (1.0 + np.exp(-diff))\n",
    "        \n",
    "        # Calculate gradients\n",
    "        grad_user = sigmoid * (self.item_factors[neg_item] - self.item_factors[pos_item]) + self.regularization * self.user_factors[user]\n",
    "        grad_pos_item = sigmoid * (-self.user_factors[user]) + self.regularization * self.item_factors[pos_item]\n",
    "        grad_neg_item = sigmoid * self.user_factors[user] + self.regularization * self.item_factors[neg_item]\n",
    "        \n",
    "        # Update factors\n",
    "        self.user_factors[user] -= self.learning_rate * grad_user\n",
    "        self.item_factors[pos_item] -= self.learning_rate * grad_pos_item\n",
    "        self.item_factors[neg_item] -= self.learning_rate * grad_neg_item\n",
    "    \n",
    "    def recommend(self, user_id, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate item recommendations for a user\n",
    "        \"\"\"\n",
    "        # Calculate predicted scores for all items\n",
    "        scores = np.dot(self.user_factors[user_id], self.item_factors.T)\n",
    "        \n",
    "        # If requested, exclude items the user has already interacted with\n",
    "        if exclude_seen and user_id in self.user_items:\n",
    "            seen_items = list(self.user_items[user_id])\n",
    "            scores[seen_items] = -np.inf\n",
    "            \n",
    "        # Get top n items by score\n",
    "        top_items = np.argsort(scores)[::-1][:n]\n",
    "        \n",
    "        return top_items\n",
    "\n",
    "#################################\n",
    "# RERANKER IMPLEMENTATION\n",
    "#################################\n",
    "\n",
    "class SimpleReranker:\n",
    "    \"\"\"\n",
    "    Simple reranker that balances original scores with diversity\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Initialize reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained BPR model\n",
    "        - alpha: weight for original scores (between 0 and 1)\n",
    "                 higher alpha means more focus on accuracy\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Calculate item popularity\n",
    "        self.item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    self.item_popularity[item] += 1\n",
    "        \n",
    "        # Normalize popularity\n",
    "        max_pop = np.max(self.item_popularity)\n",
    "        if max_pop > 0:\n",
    "            self.norm_popularity = self.item_popularity / max_pop\n",
    "        else:\n",
    "            self.norm_popularity = np.zeros_like(self.item_popularity)\n",
    "    \n",
    "    def rerank(self, user_id, n=10):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations\n",
    "        \"\"\"\n",
    "        # Get original scores for all items\n",
    "        original_scores = np.dot(self.model.user_factors[user_id], self.model.item_factors.T)\n",
    "        \n",
    "        # Exclude seen items\n",
    "        if user_id in self.model.user_items:\n",
    "            seen_items = list(self.model.user_items[user_id])\n",
    "            original_scores[seen_items] = -np.inf\n",
    "        \n",
    "        # Get candidate items (top k*3)\n",
    "        candidates = np.argsort(original_scores)[::-1][:n*3]\n",
    "        \n",
    "        # Initialize selected items and scores\n",
    "        selected = []\n",
    "        \n",
    "        # Iteratively select items\n",
    "        while len(selected) < n:\n",
    "            best_score = -np.inf\n",
    "            best_item = None\n",
    "            \n",
    "            for item in candidates:\n",
    "                if item in selected:\n",
    "                    continue\n",
    "                \n",
    "                # Original score component (normalized)\n",
    "                score_orig = original_scores[item]\n",
    "                \n",
    "                # Diversity component\n",
    "                diversity_score = 0\n",
    "                if selected:\n",
    "                    # Use item factors to calculate similarity\n",
    "                    item_factors = self.model.item_factors[item]\n",
    "                    selected_factors = self.model.item_factors[selected]\n",
    "                    \n",
    "                    # Calculate average similarity\n",
    "                    similarities = []\n",
    "                    for i, sel_factors in enumerate(selected_factors):\n",
    "                        # Cosine similarity\n",
    "                        dot_product = np.dot(item_factors, sel_factors)\n",
    "                        norm_product = np.linalg.norm(item_factors) * np.linalg.norm(sel_factors)\n",
    "                        \n",
    "                        if norm_product > 0:\n",
    "                            sim = dot_product / norm_product\n",
    "                            similarities.append(sim)\n",
    "                    \n",
    "                    if similarities:\n",
    "                        avg_sim = np.mean(similarities)\n",
    "                        diversity_score = 1 - avg_sim\n",
    "                \n",
    "                # Novelty component (inverse popularity)\n",
    "                novelty_score = 1 - self.norm_popularity[item]\n",
    "                \n",
    "                # Calculate weighted score\n",
    "                combined_score = (\n",
    "                    self.alpha * score_orig + \n",
    "                    (1 - self.alpha) * 0.5 * diversity_score + \n",
    "                    (1 - self.alpha) * 0.5 * novelty_score\n",
    "                )\n",
    "                \n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_item = item\n",
    "            \n",
    "            if best_item is None:\n",
    "                break\n",
    "                \n",
    "            selected.append(best_item)\n",
    "            \n",
    "        return selected\n",
    "\n",
    "#################################\n",
    "# EVALUATION METRICS\n",
    "#################################\n",
    "\n",
    "def calculate_ndcg(recommended_items, relevant_items, relevant_scores, k=None):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(recommended_items)\n",
    "    else:\n",
    "        k = min(k, len(recommended_items))\n",
    "    \n",
    "    # Create a dictionary mapping relevant items to their scores\n",
    "    relevance_map = {item_id: score for item_id, score in zip(relevant_items, relevant_scores)}\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = 0\n",
    "    for i, item_id in enumerate(recommended_items[:k]):\n",
    "        if item_id in relevance_map:\n",
    "            # Use rating as relevance score\n",
    "            rel = relevance_map[item_id]\n",
    "            # DCG formula: (2^rel - 1) / log2(i+2)\n",
    "            dcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Calculate ideal DCG (IDCG)\n",
    "    # Sort relevant items by their relevance scores in descending order\n",
    "    sorted_relevant = sorted(zip(relevant_items, relevant_scores), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    idcg = 0\n",
    "    for i, (item_id, rel) in enumerate(sorted_relevant[:k]):\n",
    "        # IDCG formula: (2^rel - 1) / log2(i+2)\n",
    "        idcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate NDCG\n",
    "    ndcg = dcg / idcg\n",
    "    \n",
    "    return ndcg\n",
    "\n",
    "def calculate_precision(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = num_relevant_recommended / len(recommended_items) if recommended_items else 0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def calculate_recall(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = num_relevant_recommended / len(relevant_items) if relevant_items else 0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def calculate_diversity_metrics(recommendations, item_popularity, total_items, tail_items=None):\n",
    "    \"\"\"\n",
    "    Calculate diversity metrics for a set of recommendations\n",
    "    \"\"\"\n",
    "    # Count occurrences of each item in recommendations\n",
    "    rec_counts = Counter(recommendations)\n",
    "    \n",
    "    # 1. Item Coverage\n",
    "    recommended_items = len(rec_counts)\n",
    "    item_coverage = recommended_items / total_items\n",
    "    \n",
    "    # 2. Gini Index\n",
    "    sorted_counts = sorted(rec_counts.values())\n",
    "    n = len(sorted_counts)\n",
    "    \n",
    "    if n == 0:\n",
    "        gini_index = 0\n",
    "    else:\n",
    "        cumulative_sum = 0\n",
    "        for i, count in enumerate(sorted_counts):\n",
    "            cumulative_sum += (i + 1) * count\n",
    "        \n",
    "        # Gini index formula\n",
    "        gini_index = (2 * cumulative_sum) / (n * sum(sorted_counts)) - (n + 1) / n\n",
    "    \n",
    "    # 3. Shannon Entropy\n",
    "    recommendations_count = sum(rec_counts.values())\n",
    "    probabilities = [count / recommendations_count for count in rec_counts.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    \n",
    "    # Normalize entropy\n",
    "    max_entropy = np.log2(min(total_items, recommendations_count))\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    \n",
    "    # 4. Tail Percentage\n",
    "    if tail_items is None:\n",
    "        # If tail_items not provided, use the bottom 20% by popularity\n",
    "        sorted_pop_indices = np.argsort(item_popularity)\n",
    "        num_tail_items = int(len(sorted_pop_indices) * 0.2)  # 20% least popular items\n",
    "        tail_items = set(sorted_pop_indices[:num_tail_items])\n",
    "    \n",
    "    tail_recommendations = sum(1 for item in recommendations if item in tail_items)\n",
    "    tail_percentage = tail_recommendations / len(recommendations) if recommendations else 0\n",
    "    \n",
    "    # Create results dictionary\n",
    "    metrics = {\n",
    "        'item_coverage': item_coverage,\n",
    "        'gini_index': gini_index,\n",
    "        'shannon_entropy': normalized_entropy,\n",
    "        'tail_percentage': tail_percentage\n",
    "    }\n",
    "    \n",
    "    return metrics, tail_items\n",
    "\n",
    "#################################\n",
    "# HELPER FUNCTIONS\n",
    "#################################\n",
    "\n",
    "def load_movielens_100k(path=\"ml-100k\"):\n",
    "    \"\"\"\n",
    "    Load the MovieLens 100K dataset\n",
    "    \"\"\"\n",
    "    # Load ratings\n",
    "    ratings_df = pd.read_csv(f\"{path}/u.data\", sep='\\t', \n",
    "                           names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Load movie information\n",
    "    movie_df = pd.read_csv(f\"{path}/u.item\", sep='|', encoding='latin-1',\n",
    "                          names=['item_id', 'title', 'release_date', 'video_release_date',\n",
    "                                 'IMDb_URL'] + [f'genre_{i}' for i in range(19)])\n",
    "    \n",
    "    return ratings_df, movie_df\n",
    "\n",
    "def create_user_item_matrix(ratings_df):\n",
    "    \"\"\"\n",
    "    Create a sparse user-item interaction matrix from ratings\n",
    "    \"\"\"\n",
    "    # Create mappings from original IDs to matrix indices\n",
    "    user_ids = ratings_df['user_id'].unique()\n",
    "    item_ids = ratings_df['item_id'].unique()\n",
    "    \n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(user_ids)}\n",
    "    item_mapping = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "    \n",
    "    # Map original IDs to matrix indices\n",
    "    rows = ratings_df['user_id'].map(user_mapping)\n",
    "    cols = ratings_df['item_id'].map(item_mapping)\n",
    "    \n",
    "    # Create binary matrix (1 if interaction exists, 0 otherwise)\n",
    "    data = np.ones(len(ratings_df))\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                 shape=(len(user_mapping), len(item_mapping)))\n",
    "    \n",
    "    return user_item_matrix, user_mapping, item_mapping\n",
    "\n",
    "#################################\n",
    "# COMPREHENSIVE EVALUATION\n",
    "#################################\n",
    "\n",
    "def comprehensive_evaluation(alpha=0.7, k=10, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run a comprehensive evaluation measuring both accuracy and diversity\n",
    "    \n",
    "    Parameters:\n",
    "    - alpha: weight for accuracy in reranking (0 to 1)\n",
    "    - k: number of recommendations to evaluate\n",
    "    - sample_size: number of users to sample (None for all users)\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"COMPREHENSIVE EVALUATION (alpha={alpha}, k={k})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\nLoading MovieLens 100K dataset...\")\n",
    "    ratings_df, movie_df = load_movielens_100k()\n",
    "    \n",
    "    print(\"Splitting data for evaluation...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        ratings_df, \n",
    "        test_size=0.2, \n",
    "        stratify=ratings_df['user_id'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Creating user-item matrix...\")\n",
    "    user_item_matrix, user_mapping, item_mapping = create_user_item_matrix(train_df)\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    reverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "    reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    \n",
    "    # Create test set ground truth\n",
    "    test_relevant_items = defaultdict(list)\n",
    "    test_relevant_scores = defaultdict(list)\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Only include users and items that exist in our mappings\n",
    "        if user_id in user_mapping and item_id in item_mapping:\n",
    "            test_relevant_items[user_id].append(item_id)\n",
    "            test_relevant_scores[user_id].append(rating)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining BPR model...\")\n",
    "    model = BPRRecommender(factors=50, learning_rate=0.01, \n",
    "                          regularization=0.01, iterations=30)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    # Initialize reranker\n",
    "    print(\"\\nInitializing reranker with alpha =\", alpha)\n",
    "    reranker = SimpleReranker(model=model, alpha=alpha)\n",
    "    \n",
    "    # Select users for evaluation\n",
    "    if sample_size is not None and sample_size < len(test_relevant_items):\n",
    "        eval_users = random.sample(list(test_relevant_items.keys()), sample_size)\n",
    "    else:\n",
    "        eval_users = list(test_relevant_items.keys())\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(eval_users)} users...\")\n",
    "    \n",
    "    # Initialize metrics collectors\n",
    "    original_ndcg = []\n",
    "    original_precision = []\n",
    "    original_recall = []\n",
    "    original_recs = []\n",
    "    \n",
    "    reranked_ndcg = []\n",
    "    reranked_precision = []\n",
    "    reranked_recall = []\n",
    "    reranked_recs = []\n",
    "    \n",
    "    # Evaluate each user\n",
    "    for user_id in eval_users:\n",
    "        # Skip if user has no relevant items\n",
    "        if not test_relevant_items[user_id]:\n",
    "            continue\n",
    "        \n",
    "        user_idx = user_mapping[user_id]\n",
    "        \n",
    "        # Get original recommendations\n",
    "        original_rec_idx = model.recommend(user_idx, n=k)\n",
    "        original_rec = [reverse_item_mapping[idx] for idx in original_rec_idx]\n",
    "        original_recs.extend(original_rec_idx)\n",
    "        \n",
    "        # Calculate accuracy metrics for original\n",
    "        original_ndcg.append(calculate_ndcg(\n",
    "            original_rec, test_relevant_items[user_id], test_relevant_scores[user_id]\n",
    "        ))\n",
    "        original_precision.append(calculate_precision(\n",
    "            original_rec, test_relevant_items[user_id]\n",
    "        ))\n",
    "        original_recall.append(calculate_recall(\n",
    "            original_rec, test_relevant_items[user_id]\n",
    "        ))\n",
    "        \n",
    "        # Get reranked recommendations\n",
    "        reranked_rec_idx = reranker.rerank(user_idx, n=k)\n",
    "        reranked_rec = [reverse_item_mapping[idx] for idx in reranked_rec_idx]\n",
    "        reranked_recs.extend(reranked_rec_idx)\n",
    "        \n",
    "        # Calculate accuracy metrics for reranked\n",
    "        reranked_ndcg.append(calculate_ndcg(\n",
    "            reranked_rec, test_relevant_items[user_id], test_relevant_scores[user_id]\n",
    "        ))\n",
    "        reranked_precision.append(calculate_precision(\n",
    "            reranked_rec, test_relevant_items[user_id]\n",
    "        ))\n",
    "        reranked_recall.append(calculate_recall(\n",
    "            reranked_rec, test_relevant_items[user_id]\n",
    "        ))\n",
    "    \n",
    "    # Calculate average accuracy metrics\n",
    "    orig_accuracy = {\n",
    "        f'ndcg@{k}': np.mean(original_ndcg),\n",
    "        f'precision@{k}': np.mean(original_precision),\n",
    "        f'recall@{k}': np.mean(original_recall)\n",
    "    }\n",
    "    \n",
    "    rerank_accuracy = {\n",
    "        f'ndcg@{k}': np.mean(reranked_ndcg),\n",
    "        f'precision@{k}': np.mean(reranked_precision),\n",
    "        f'recall@{k}': np.mean(reranked_recall)\n",
    "    }\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    # First calculate item popularity\n",
    "    item_popularity = np.zeros(model.n_items)\n",
    "    for user in range(model.n_users):\n",
    "        if user in model.user_items:\n",
    "            for item in model.user_items[user]:\n",
    "                item_popularity[item] += 1\n",
    "    \n",
    "    # Then calculate diversity metrics\n",
    "    orig_diversity, tail_items = calculate_diversity_metrics(\n",
    "        recommendations=original_recs,\n",
    "        item_popularity=item_popularity,\n",
    "        total_items=model.n_items\n",
    "    )\n",
    "    \n",
    "    rerank_diversity, _ = calculate_diversity_metrics(\n",
    "        recommendations=reranked_recs,\n",
    "        item_popularity=item_popularity,\n",
    "        total_items=model.n_items,\n",
    "        tail_items=tail_items\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*30 + \" ACCURACY METRICS \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15} {'Original':<15} {'Reranked':<15} {'Change (%)':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for metric in [f'ndcg@{k}', f'precision@{k}', f'recall@{k}']:\n",
    "        orig = orig_accuracy[metric]\n",
    "        rerank = rerank_accuracy[metric]\n",
    "        change = ((rerank - orig) / orig) * 100 if orig > 0 else float('inf')\n",
    "        print(f\"{metric:<15} {orig:.4f}{' '*10} {rerank:.4f}{' '*10} {change:+.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \" DIVERSITY METRICS \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<20} {'Original':<15} {'Reranked':<15} {'Change (%)':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for metric in ['item_coverage', 'gini_index', 'shannon_entropy', 'tail_percentage']:\n",
    "        orig = orig_diversity[metric]\n",
    "        rerank = rerank_diversity[metric]\n",
    "        change = ((rerank - orig) / orig) * 100 if orig > 0 else float('inf')\n",
    "        print(f\"{metric:<20} {orig:.4f}{' '*10} {rerank:.4f}{' '*10} {change:+.2f}%\")\n",
    "    \n",
    "    # Print interpretations\n",
    "    print(\"\\n\" + \"=\"*30 + \" METRIC INTERPRETATIONS \" + \"=\"*30)\n",
    "    print(\"Accuracy Metrics:\")\n",
    "    print(\"- NDCG: Higher is better, measures ranking quality\")\n",
    "    print(\"- Precision: Higher is better, measures relevant item ratio in recommendations\")\n",
    "    print(\"- Recall: Higher is better, measures coverage of all relevant items\")\n",
    "    \n",
    "    print(\"\\nDiversity Metrics:\")\n",
    "    print(\"- Item Coverage: Higher means more catalog items are recommended\")\n",
    "    print(\"- Gini Index: Lower means more equality in item recommendations\")\n",
    "    print(\"- Shannon Entropy: Higher means more diverse recommendations\")\n",
    "    print(\"- Tail Percentage: Higher means more niche items are recommended\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'original': {\n",
    "            'accuracy': orig_accuracy,\n",
    "            'diversity': orig_diversity\n",
    "        },\n",
    "        'reranked': {\n",
    "            'accuracy': rerank_accuracy,\n",
    "            'diversity': rerank_diversity\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run evaluation with default parameters\n",
    "    comprehensive_evaluation(alpha=0.7, k=10)\n",
    "    \n",
    "    # Uncomment to try different alpha values\n",
    "    # comprehensive_evaluation(alpha=0.5, k=10)  # Equal weight to accuracy and diversity\n",
    "    # comprehensive_evaluation(alpha=0.9, k=10)  # Strong focus on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415c9c6f-7153-49d5-8af8-96a3bf319a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k=10)\n",
      "================================================================================\n",
      "\n",
      "Loading MovieLens 100K dataset...\n",
      "Splitting data for evaluation...\n",
      "Creating user-item matrix...\n",
      "\n",
      "Training BPR model...\n",
      "Completed iteration 10/30\n",
      "Completed iteration 20/30\n",
      "Completed iteration 30/30\n",
      "\n",
      "Initializing rerankers...\n",
      "\n",
      "Evaluating 943 users...\n",
      "\n",
      "Evaluating Original BPR...\n",
      "\n",
      "Evaluating Simple Reranker...\n",
      "\n",
      "Evaluating MMR Reranker...\n",
      "\n",
      "============================== ACCURACY METRICS COMPARISON ==============================\n",
      "Metric         Original BPR        Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "ndcg@10        0.2806               0.2775 (-1.1%)     0.2822 (+0.5%)     \n",
      "precision@10   0.3060               0.3055 (-0.2%)     0.3077 (+0.6%)     \n",
      "recall@10      0.1992               0.1973 (-1.0%)     0.1999 (+0.3%)     \n",
      "\n",
      "============================== DIVERSITY METRICS COMPARISON ==============================\n",
      "Metric         Original BPR        Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "item_coverage  0.1564               0.1655 (+5.8%)     0.1667 (+6.6%)     \n",
      "gini_index     0.6960               0.6882 (-1.1%)     0.7005 (+0.6%)     \n",
      "shannon_entropy0.6300               0.6410 (+1.8%)     0.6365 (+1.0%)     \n",
      "tail_percentage0.0000               0.0000 (+inf%)     0.0000 (+inf%)     \n",
      "\n",
      "============================== METRIC INTERPRETATIONS ==============================\n",
      "Accuracy Metrics:\n",
      "- NDCG: Higher is better, measures ranking quality\n",
      "- Precision: Higher is better, measures relevant item ratio in recommendations\n",
      "- Recall: Higher is better, measures coverage of all relevant items\n",
      "\n",
      "Diversity Metrics:\n",
      "- Item Coverage: Higher means more catalog items are recommended\n",
      "- Gini Index: Lower means more equality in item recommendations\n",
      "- Shannon Entropy: Higher means more diverse recommendations\n",
      "- Tail Percentage: Higher means more niche items are recommended\n"
     ]
    }
   ],
   "source": [
    "class MMRReranker:\n",
    "    \"\"\"\n",
    "    Maximum Marginal Relevance (MMR) Reranker\n",
    "    \n",
    "    This reranker balances between relevance and diversity explicitly by\n",
    "    selecting items that maximize marginal relevance - items that are\n",
    "    both relevant to the user and different from already selected items.\n",
    "    \n",
    "    MMR formula: MMR = Œª * rel(i) - (1-Œª) * max(sim(i,j)) for j in selected items\n",
    "    \n",
    "    Where:\n",
    "    - rel(i) is the relevance of item i to the user\n",
    "    - sim(i,j) is the similarity between items i and j\n",
    "    - Œª is a parameter that controls the trade-off between relevance and diversity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, lambda_param=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the MMR reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained BPR model\n",
    "        - lambda_param: trade-off parameter between relevance and diversity (0-1)\n",
    "                        higher values favor relevance, lower values favor diversity\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.lambda_param = lambda_param\n",
    "        \n",
    "    def calculate_item_similarity(self, item1, item2):\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two items based on their latent factors\n",
    "        \n",
    "        Parameters:\n",
    "        - item1: index of first item\n",
    "        - item2: index of second item\n",
    "        \n",
    "        Returns:\n",
    "        - similarity: cosine similarity between items (-1 to 1)\n",
    "        \"\"\"\n",
    "        v1 = self.model.item_factors[item1]\n",
    "        v2 = self.model.item_factors[item2]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        norm_product = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "        \n",
    "        if norm_product == 0:\n",
    "            return 0\n",
    "        \n",
    "        return dot_product / norm_product\n",
    "    \n",
    "    def rerank(self, user_id, n=10, candidate_size=100):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations using Maximum Marginal Relevance\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index in the model\n",
    "        - n: number of recommendations to return\n",
    "        - candidate_size: number of initial candidates to consider\n",
    "        \n",
    "        Returns:\n",
    "        - reranked_items: list of reranked item indices\n",
    "        \"\"\"\n",
    "        # Get original relevance scores for all items\n",
    "        relevance_scores = np.dot(self.model.user_factors[user_id], self.model.item_factors.T)\n",
    "        \n",
    "        # Exclude seen items\n",
    "        if user_id in self.model.user_items:\n",
    "            seen_items = list(self.model.user_items[user_id])\n",
    "            relevance_scores[seen_items] = -np.inf\n",
    "        \n",
    "        # Get initial candidates\n",
    "        candidates = np.argsort(relevance_scores)[::-1][:candidate_size]\n",
    "        \n",
    "        # Normalize relevance scores to [0,1] range for the candidates\n",
    "        candidate_scores = relevance_scores[candidates]\n",
    "        min_score = np.min(candidate_scores)\n",
    "        max_score = np.max(candidate_scores)\n",
    "        score_range = max_score - min_score\n",
    "        \n",
    "        if score_range > 0:\n",
    "            normalized_scores = (candidate_scores - min_score) / score_range\n",
    "        else:\n",
    "            normalized_scores = np.zeros_like(candidate_scores)\n",
    "        \n",
    "        # Initialize selected items\n",
    "        selected = []\n",
    "        \n",
    "        # Select first item (most relevant)\n",
    "        selected.append(candidates[np.argmax(normalized_scores)])\n",
    "        remaining_candidates = set(candidates) - set(selected)\n",
    "        \n",
    "        # Iteratively select items using MMR\n",
    "        while len(selected) < n and remaining_candidates:\n",
    "            max_mmr = -np.inf\n",
    "            max_item = None\n",
    "            \n",
    "            for item in remaining_candidates:\n",
    "                # Get relevance component\n",
    "                item_idx = np.where(candidates == item)[0][0]\n",
    "                relevance = normalized_scores[item_idx]\n",
    "                \n",
    "                # Calculate diversity component (inverse of maximum similarity)\n",
    "                max_sim = 0\n",
    "                for selected_item in selected:\n",
    "                    sim = self.calculate_item_similarity(item, selected_item)\n",
    "                    max_sim = max(max_sim, sim)\n",
    "                \n",
    "                # Calculate MMR score\n",
    "                mmr_score = self.lambda_param * relevance - (1 - self.lambda_param) * max_sim\n",
    "                \n",
    "                if mmr_score > max_mmr:\n",
    "                    max_mmr = mmr_score\n",
    "                    max_item = item\n",
    "            \n",
    "            if max_item is not None:\n",
    "                selected.append(max_item)\n",
    "                remaining_candidates.remove(max_item)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return selected\n",
    "\n",
    "# Add this to comprehensive evaluation to test multiple rerankers\n",
    "def comprehensive_evaluation_multiple_rerankers(k=10, sample_size=None):\n",
    "    \"\"\"\n",
    "    Run a comprehensive evaluation measuring both accuracy and diversity for multiple rerankers\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k={k})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\nLoading MovieLens 100K dataset...\")\n",
    "    ratings_df, movie_df = load_movielens_100k()\n",
    "    \n",
    "    print(\"Splitting data for evaluation...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        ratings_df, \n",
    "        test_size=0.2, \n",
    "        stratify=ratings_df['user_id'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Creating user-item matrix...\")\n",
    "    user_item_matrix, user_mapping, item_mapping = create_user_item_matrix(train_df)\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    reverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "    reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    \n",
    "    # Create test set ground truth\n",
    "    test_relevant_items = defaultdict(list)\n",
    "    test_relevant_scores = defaultdict(list)\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Only include users and items that exist in our mappings\n",
    "        if user_id in user_mapping and item_id in item_mapping:\n",
    "            test_relevant_items[user_id].append(item_id)\n",
    "            test_relevant_scores[user_id].append(rating)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining BPR model...\")\n",
    "    model = BPRRecommender(factors=50, learning_rate=0.01, \n",
    "                          regularization=0.01, iterations=30)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    # Initialize rerankers\n",
    "    print(\"\\nInitializing rerankers...\")\n",
    "    simple_reranker = SimpleReranker(model=model, alpha=0.7)\n",
    "    mmr_reranker = MMRReranker(model=model, lambda_param=0.7)\n",
    "    \n",
    "    # Setup dictionary for all rerankers' results\n",
    "    rerankers = {\n",
    "        \"Original BPR\": None,\n",
    "        \"Simple Reranker\": simple_reranker,\n",
    "        \"MMR Reranker\": mmr_reranker\n",
    "    }\n",
    "    \n",
    "    # Results dictionary\n",
    "    all_results = {}\n",
    "    \n",
    "    # Select users for evaluation\n",
    "    if sample_size is not None and sample_size < len(test_relevant_items):\n",
    "        eval_users = random.sample(list(test_relevant_items.keys()), sample_size)\n",
    "    else:\n",
    "        eval_users = list(test_relevant_items.keys())\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(eval_users)} users...\")\n",
    "    \n",
    "    # Evaluate each reranker\n",
    "    for reranker_name, reranker in rerankers.items():\n",
    "        print(f\"\\nEvaluating {reranker_name}...\")\n",
    "        \n",
    "        # Initialize metrics collectors\n",
    "        ndcg_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        all_recs = []\n",
    "        \n",
    "        # Evaluate each user\n",
    "        for user_id in eval_users:\n",
    "            # Skip if user has no relevant items\n",
    "            if not test_relevant_items[user_id]:\n",
    "                continue\n",
    "            \n",
    "            user_idx = user_mapping[user_id]\n",
    "            \n",
    "            # Get recommendations\n",
    "            if reranker is None:  # Original BPR\n",
    "                rec_idx = model.recommend(user_idx, n=k)\n",
    "            else:  # Use reranker\n",
    "                rec_idx = reranker.rerank(user_idx, n=k)\n",
    "                \n",
    "            rec = [reverse_item_mapping[idx] for idx in rec_idx]\n",
    "            all_recs.extend(rec_idx)\n",
    "            \n",
    "            # Calculate accuracy metrics\n",
    "            ndcg_scores.append(calculate_ndcg(\n",
    "                rec, test_relevant_items[user_id], test_relevant_scores[user_id]\n",
    "            ))\n",
    "            precision_scores.append(calculate_precision(\n",
    "                rec, test_relevant_items[user_id]\n",
    "            ))\n",
    "            recall_scores.append(calculate_recall(\n",
    "                rec, test_relevant_items[user_id]\n",
    "            ))\n",
    "        \n",
    "        # Calculate average accuracy metrics\n",
    "        accuracy_metrics = {\n",
    "            f'ndcg@{k}': np.mean(ndcg_scores),\n",
    "            f'precision@{k}': np.mean(precision_scores),\n",
    "            f'recall@{k}': np.mean(recall_scores)\n",
    "        }\n",
    "        \n",
    "        # Calculate diversity metrics\n",
    "        # First calculate item popularity\n",
    "        item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    item_popularity[item] += 1\n",
    "        \n",
    "        # Then calculate diversity metrics\n",
    "        diversity_metrics, _ = calculate_diversity_metrics(\n",
    "            recommendations=all_recs,\n",
    "            item_popularity=item_popularity,\n",
    "            total_items=model.n_items\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[reranker_name] = {\n",
    "            'accuracy': accuracy_metrics,\n",
    "            'diversity': diversity_metrics\n",
    "        }\n",
    "    \n",
    "    # Print comparative results\n",
    "    print(\"\\n\" + \"=\"*30 + \" ACCURACY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for reranker_name in rerankers.keys():\n",
    "        print(f\"{reranker_name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in [f'ndcg@{k}', f'precision@{k}', f'recall@{k}']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original BPR\"]['accuracy'][metric]\n",
    "        for reranker_name in rerankers.keys():\n",
    "            value = all_results[reranker_name]['accuracy'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            \n",
    "            if reranker_name == \"Original BPR\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \" DIVERSITY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for reranker_name in rerankers.keys():\n",
    "        print(f\"{reranker_name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for metric in ['item_coverage', 'gini_index', 'shannon_entropy', 'tail_percentage']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original BPR\"]['diversity'][metric]\n",
    "        for reranker_name in rerankers.keys():\n",
    "            value = all_results[reranker_name]['diversity'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            \n",
    "            if reranker_name == \"Original BPR\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    # Print interpretations\n",
    "    print(\"\\n\" + \"=\"*30 + \" METRIC INTERPRETATIONS \" + \"=\"*30)\n",
    "    print(\"Accuracy Metrics:\")\n",
    "    print(\"- NDCG: Higher is better, measures ranking quality\")\n",
    "    print(\"- Precision: Higher is better, measures relevant item ratio in recommendations\")\n",
    "    print(\"- Recall: Higher is better, measures coverage of all relevant items\")\n",
    "    \n",
    "    print(\"\\nDiversity Metrics:\")\n",
    "    print(\"- Item Coverage: Higher means more catalog items are recommended\")\n",
    "    print(\"- Gini Index: Lower means more equality in item recommendations\")\n",
    "    print(\"- Shannon Entropy: Higher means more diverse recommendations\")\n",
    "    print(\"- Tail Percentage: Higher means more niche items are recommended\")\n",
    "    \n",
    "    # Return all results\n",
    "    return all_results\n",
    "\n",
    "# Execute with multiple rerankers when running the script directly\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_evaluation_multiple_rerankers(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dc8e8-f12e-4e3e-870d-b4c4c770d989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
