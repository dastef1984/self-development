{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88040539-27db-465c-a657-7abb5353f051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k=10)\n",
      "================================================================================\n",
      "\n",
      "Loading LastFM dataset...\n",
      "Loading LastFM dataset...\n",
      "Trying to load LastFM data with utf-8 encoding...\n",
      "Successfully loaded LastFM dataset with utf-8 encoding\n",
      "Loaded 92835 interactions from 1893 users on 17633 artists\n",
      "Splitting data for evaluation...\n",
      "Using random sampling (some users have only 1 rating)...\n",
      "Creating user-item matrix...\n",
      "\n",
      "Training NeuMF model...\n",
      "Training NeuMF model for 10 epochs...\n",
      "Epoch 1/10, Loss: 0.3847, Time: 3.95s\n",
      "Epoch 5/10, Loss: 0.1355, Time: 3.86s\n",
      "Epoch 10/10, Loss: 0.0497, Time: 3.05s\n",
      "\n",
      "Initializing rerankers...\n",
      "\n",
      "Evaluating 1870 users...\n",
      "\n",
      "Evaluating Original NeuMF...\n",
      "\n",
      "Evaluating Simple Reranker...\n",
      "\n",
      "Evaluating MMR Reranker...\n",
      "\n",
      "============================== ACCURACY METRICS COMPARISON ==============================\n",
      "Metric         Original NeuMF      Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "ndcg@10        0.1859               0.0545 (-70.7%)     0.1420 (-23.6%)     \n",
      "precision@10   0.1388               0.0478 (-65.5%)     0.1041 (-25.0%)     \n",
      "recall@10      0.1548               0.0541 (-65.0%)     0.1172 (-24.3%)     \n",
      "\n",
      "============================== DIVERSITY METRICS COMPARISON ==============================\n",
      "Metric         Original NeuMF      Simple Reranker     MMR Reranker        \n",
      "--------------------------------------------------------------------------------\n",
      "item_coverage  0.1066               0.1527 (+43.2%)     0.1324 (+24.2%)     \n",
      "gini_index     0.7557               0.6474 (-14.3%)     0.6928 (-8.3%)     \n",
      "shannon_entropy0.6335               0.7228 (+14.1%)     0.6875 (+8.5%)     \n",
      "tail_percentage0.0012               0.0011 (-13.0%)     0.0007 (-43.5%)     \n",
      "\n",
      "============================== METRIC INTERPRETATIONS ==============================\n",
      "Accuracy Metrics:\n",
      "- NDCG: Higher is better, measures ranking quality\n",
      "- Precision: Higher is better, measures relevant item ratio\n",
      "- Recall: Higher is better, measures coverage of relevant items\n",
      "\n",
      "Diversity Metrics:\n",
      "- Item Coverage: Higher means more catalog items are recommended\n",
      "- Gini Index: Lower indicates more equality in recommendations\n",
      "- Shannon Entropy: Higher means more diverse recommendations\n",
      "- Tail Percentage: Higher means more niche items are recommended\n"
     ]
    }
   ],
   "source": [
    "# NeuMF Recommender with Diversity Reranking for LastFM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#################################\n",
    "# DATASET AND HELPER FUNCTIONS (LASTFM)\n",
    "#################################\n",
    "\n",
    "def load_lastfm(path=\"lastfm/lastfm.inter\"):\n",
    "    \"\"\"\n",
    "    Load the LastFM dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - path: path to the lastfm.inter file\n",
    "    \n",
    "    Returns:\n",
    "    - ratings_df: DataFrame with columns ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    - dummy_df: Empty DataFrame with item information structure (for compatibility)\n",
    "    \"\"\"\n",
    "    print(\"Loading LastFM dataset...\")\n",
    "    encodings_to_try = ['utf-8', 'latin-1', 'ISO-8859-1', 'cp1252']\n",
    "    \n",
    "    def generate_sample_data():\n",
    "        print(\"Generating sample LastFM data for demonstration purposes...\")\n",
    "        np.random.seed(42)\n",
    "        n_users = 100\n",
    "        n_items = 50\n",
    "        n_ratings = 1000\n",
    "        user_ids = [f\"user_{i}\" for i in range(n_users)]\n",
    "        item_ids = [f\"artist_{i}\" for i in range(n_items)]\n",
    "        random_users = np.random.choice(user_ids, size=n_ratings)\n",
    "        random_items = np.random.choice(item_ids, size=n_ratings)\n",
    "        random_ratings = np.random.uniform(1, 5, size=n_ratings)\n",
    "        constant_timestamp = np.full(n_ratings, 1111111111)\n",
    "        sample_df = pd.DataFrame({\n",
    "            'user_id': random_users,\n",
    "            'item_id': random_items,\n",
    "            'rating': random_ratings,\n",
    "            'timestamp': constant_timestamp\n",
    "        })\n",
    "        dummy_df = pd.DataFrame(columns=['item_id', 'name', 'tags'])\n",
    "        print(f\"Generated sample data with {len(sample_df)} ratings from {sample_df['user_id'].nunique()} users on {sample_df['item_id'].nunique()} artists\")\n",
    "        return sample_df, dummy_df\n",
    "    \n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            print(f\"Trying to load LastFM data with {encoding} encoding...\")\n",
    "            columns = ['user_id', 'artist_id', 'weight', 'tag_value']\n",
    "            raw_df = pd.read_csv(path, sep='\\t', names=columns, encoding=encoding)\n",
    "            raw_df = raw_df.rename(columns={'artist_id': 'item_id', 'weight': 'rating'})\n",
    "            raw_df['timestamp'] = 1111111111\n",
    "            raw_df = raw_df.drop(columns=['tag_value'])\n",
    "            dummy_df = pd.DataFrame(columns=['item_id', 'name', 'tags'])\n",
    "            if len(raw_df) > 0:\n",
    "                print(f\"Successfully loaded LastFM dataset with {encoding} encoding\")\n",
    "                print(f\"Loaded {len(raw_df)} interactions from {raw_df['user_id'].nunique()} users on {raw_df['item_id'].nunique()} artists\")\n",
    "                return raw_df, dummy_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading LastFM dataset with {encoding} encoding: {str(e)}\")\n",
    "            continue\n",
    "    print(\"All attempts to load the LastFM dataset failed. Generating sample data instead.\")\n",
    "    return generate_sample_data()\n",
    "\n",
    "def create_user_item_matrix(ratings_df):\n",
    "    \"\"\"\n",
    "    Create a sparse user-item interaction matrix from ratings\n",
    "    \"\"\"\n",
    "    user_ids = ratings_df['user_id'].unique()\n",
    "    item_ids = ratings_df['item_id'].unique()\n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(user_ids)}\n",
    "    item_mapping = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "    rows = ratings_df['user_id'].map(user_mapping)\n",
    "    cols = ratings_df['item_id'].map(item_mapping)\n",
    "    data = np.ones(len(ratings_df))\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                  shape=(len(user_mapping), len(item_mapping)))\n",
    "    return user_item_matrix, user_mapping, item_mapping\n",
    "\n",
    "class NCFDataset(Dataset):\n",
    "    \"\"\"Dataset for NeuMF training\"\"\"\n",
    "    def __init__(self, user_item_matrix, neg_samples=4):\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.users, self.items = user_item_matrix.nonzero()\n",
    "        self.n_users = user_item_matrix.shape[0]\n",
    "        self.n_items = user_item_matrix.shape[1]\n",
    "        self.neg_samples = neg_samples\n",
    "        self.user_item_set = set(zip(self.users, self.items))\n",
    "        self.user_items = defaultdict(set)\n",
    "        for u, i in zip(self.users, self.items):\n",
    "            self.user_items[u].add(i)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.users) * (1 + self.neg_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.users):\n",
    "            user = self.users[idx]\n",
    "            item = self.items[idx]\n",
    "            label = 1.0\n",
    "        else:\n",
    "            pos_idx = idx % len(self.users)\n",
    "            user = self.users[pos_idx]\n",
    "            item = random.randint(0, self.n_items - 1)\n",
    "            while item in self.user_items[user]:\n",
    "                item = random.randint(0, self.n_items - 1)\n",
    "            label = 0.0\n",
    "        return user, item, label\n",
    "\n",
    "#################################\n",
    "# NEUMF RECOMMENDER IMPLEMENTATION\n",
    "#################################\n",
    "\n",
    "class GMF(nn.Module):\n",
    "    \"\"\"Generalized Matrix Factorization\"\"\"\n",
    "    def __init__(self, n_users, n_items, latent_dim):\n",
    "        super(GMF, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, latent_dim)\n",
    "        self.output_layer = nn.Linear(latent_dim, 1)\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_emb = self.user_embedding(user_indices)\n",
    "        item_emb = self.item_embedding(item_indices)\n",
    "        element_product = torch.mul(user_emb, item_emb)\n",
    "        output = self.output_layer(element_product)\n",
    "        return output.view(-1)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron component\"\"\"\n",
    "    def __init__(self, n_users, n_items, latent_dim, layers=[64, 32, 16, 8]):\n",
    "        super(MLP, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, latent_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        layer_dims = [2 * latent_dim] + layers\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            self.layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.output_layer = nn.Linear(layer_dims[-1], 1)\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_emb = self.user_embedding(user_indices)\n",
    "        item_emb = self.item_embedding(item_indices)\n",
    "        vector = torch.cat([user_emb, item_emb], dim=-1)\n",
    "        for layer in self.layers:\n",
    "            vector = layer(vector)\n",
    "        output = self.output_layer(vector)\n",
    "        return output.view(-1)\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    \"\"\"Neural Matrix Factorization combining GMF and MLP\"\"\"\n",
    "    def __init__(self, n_users, n_items, latent_dim=32, mlp_layers=[64, 32, 16, 8]):\n",
    "        super(NeuMF, self).__init__()\n",
    "        self.gmf = GMF(n_users, n_items, latent_dim)\n",
    "        self.mlp = MLP(n_users, n_items, latent_dim, mlp_layers)\n",
    "        self.output_layer = nn.Linear(mlp_layers[-1] + latent_dim, 1)\n",
    "        nn.init.normal_(self.output_layer.weight, std=0.01)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # GMF path\n",
    "        gmf_user = self.gmf.user_embedding(user_indices)\n",
    "        gmf_item = self.gmf.item_embedding(item_indices)\n",
    "        gmf_vector = torch.mul(gmf_user, gmf_item)\n",
    "        # MLP path\n",
    "        mlp_user = self.mlp.user_embedding(user_indices)\n",
    "        mlp_item = self.mlp.item_embedding(item_indices)\n",
    "        mlp_vector = torch.cat([mlp_user, mlp_item], dim=-1)\n",
    "        for layer in self.mlp.layers:\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "        # Concatenate both paths\n",
    "        vector = torch.cat([gmf_vector, mlp_vector], dim=-1)\n",
    "        output = self.output_layer(vector)\n",
    "        return torch.sigmoid(output.view(-1))\n",
    "\n",
    "class NeuMFRecommender:\n",
    "    def __init__(self, latent_dim=32, mlp_layers=[64, 32, 16, 8], epochs=20, batch_size=256, \n",
    "                 lr=0.001, neg_samples=4, device=None, random_state=42):\n",
    "        \"\"\"\n",
    "        NeuMF recommender using Neural Matrix Factorization\n",
    "        \n",
    "        Parameters:\n",
    "        - latent_dim: size of latent factors\n",
    "        - mlp_layers: list defining the MLP architecture\n",
    "        - epochs: training epochs\n",
    "        - batch_size: batch size\n",
    "        - lr: learning rate\n",
    "        - neg_samples: negative samples per positive instance\n",
    "        - device: torch device (cpu or cuda)\n",
    "        - random_state: seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.latent_dim = latent_dim\n",
    "        self.mlp_layers = mlp_layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.neg_samples = neg_samples\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        random.seed(random_state)\n",
    "        np.random.seed(random_state)\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Train the NeuMF model on the provided user-item matrix.\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.n_users, self.n_items = user_item_matrix.shape\n",
    "        \n",
    "        # Build user-item interactions dictionary\n",
    "        self.user_items = defaultdict(set)\n",
    "        for u, i in zip(*self.user_item_matrix.nonzero()):\n",
    "            self.user_items[u].add(i)\n",
    "        \n",
    "        # Create training dataset and dataloader\n",
    "        dataset = NCFDataset(user_item_matrix, neg_samples=self.neg_samples)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize NeuMF model\n",
    "        self.model = NeuMF(self.n_users, self.n_items, self.latent_dim, self.mlp_layers).to(self.device)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        print(f\"Training NeuMF model for {self.epochs} epochs...\")\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            for users, items, labels in dataloader:\n",
    "                users = users.to(self.device)\n",
    "                items = items.to(self.device)\n",
    "                labels = labels.float().to(self.device)\n",
    "                outputs = self.model(users, items)\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {running_loss/len(dataloader):.4f}, Time: {elapsed_time:.2f}s\")\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Precompute embeddings for reranker compatibility\n",
    "        with torch.no_grad():\n",
    "            self.user_gmf_embeddings = self.model.gmf.user_embedding.weight.data\n",
    "            self.item_gmf_embeddings = self.model.gmf.item_embedding.weight.data\n",
    "            self.user_mlp_embeddings = self.model.mlp.user_embedding.weight.data\n",
    "            self.item_mlp_embeddings = self.model.mlp.item_embedding.weight.data\n",
    "        # Combine embeddings as item factors\n",
    "        self.item_factors = np.concatenate([\n",
    "            self.item_gmf_embeddings.cpu().numpy(),\n",
    "            self.item_mlp_embeddings.cpu().numpy()\n",
    "        ], axis=1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def recommend(self, user_id, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate recommendations for a user.\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_items:\n",
    "            all_items = list(range(self.n_items))\n",
    "            recommendations = random.sample(all_items, min(n, len(all_items)))\n",
    "            return np.array(recommendations)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_id] * self.n_items).to(self.device)\n",
    "            item_tensor = torch.LongTensor(list(range(self.n_items))).to(self.device)\n",
    "            scores = self.model(user_tensor, item_tensor).cpu().numpy()\n",
    "        \n",
    "        if exclude_seen:\n",
    "            for i in self.user_items[user_id]:\n",
    "                scores[i] = -np.inf\n",
    "        top_items = np.argsort(scores)[::-1][:n]\n",
    "        return top_items\n",
    "\n",
    "#################################\n",
    "# RERANKER IMPLEMENTATION (for NeuMF)\n",
    "#################################\n",
    "\n",
    "class SimpleRerankerNeuMF:\n",
    "    \"\"\"\n",
    "    Simple reranker for NeuMF that balances original scores with diversity.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Initialize reranker.\n",
    "        Parameters:\n",
    "        - model: trained NeuMFRecommender\n",
    "        - alpha: weight for original scores (0 to 1)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    self.item_popularity[item] += 1\n",
    "        max_pop = np.max(self.item_popularity)\n",
    "        if max_pop > 0:\n",
    "            self.norm_popularity = self.item_popularity / max_pop\n",
    "        else:\n",
    "            self.norm_popularity = np.zeros_like(self.item_popularity)\n",
    "    \n",
    "    def rerank(self, user_id, n=10):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations using NeuMF scores.\n",
    "        \"\"\"\n",
    "        # Use NeuMF's recommend method to get a candidate pool\n",
    "        candidates = self.model.recommend(user_id, n=n*3, exclude_seen=True)\n",
    "        # Get predicted scores from the NeuMF model\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_id] * self.model.n_items).to(self.model.device)\n",
    "            item_tensor = torch.LongTensor(list(range(self.model.n_items))).to(self.model.device)\n",
    "            scores = self.model.model(user_tensor, item_tensor).cpu().numpy()\n",
    "        selected = []\n",
    "        while len(selected) < n and candidates.size > 0:\n",
    "            best_score = -np.inf\n",
    "            best_item = None\n",
    "            for item in candidates:\n",
    "                if item in selected:\n",
    "                    continue\n",
    "                score_orig = scores[item]\n",
    "                diversity_score = 0\n",
    "                if selected:\n",
    "                    item_factors = self.model.item_factors[item]\n",
    "                    selected_factors = self.model.item_factors[selected]\n",
    "                    similarities = []\n",
    "                    for sel in selected_factors:\n",
    "                        dot_product = np.dot(item_factors, sel)\n",
    "                        norm_product = np.linalg.norm(item_factors) * np.linalg.norm(sel)\n",
    "                        sim = dot_product / norm_product if norm_product > 0 else 0\n",
    "                        similarities.append(sim)\n",
    "                    if similarities:\n",
    "                        avg_sim = np.mean(similarities)\n",
    "                        diversity_score = 1 - avg_sim\n",
    "                novelty_score = 1 - self.norm_popularity[item]\n",
    "                combined_score = (self.alpha * score_orig +\n",
    "                                  (1 - self.alpha) * 0.5 * diversity_score +\n",
    "                                  (1 - self.alpha) * 0.5 * novelty_score)\n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_item = item\n",
    "            if best_item is None:\n",
    "                break\n",
    "            selected.append(best_item)\n",
    "            candidates = candidates[candidates != best_item]\n",
    "        return np.array(selected)\n",
    "\n",
    "class MMRRerankerNeuMF:\n",
    "    \"\"\"\n",
    "    MMR Reranker for NeuMF.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, lambda_param=0.7):\n",
    "        self.model = model\n",
    "        self.lambda_param = lambda_param\n",
    "        \n",
    "    def calculate_item_similarity(self, item1, item2):\n",
    "        factors1 = self.model.item_factors[item1]\n",
    "        factors2 = self.model.item_factors[item2]\n",
    "        dot = np.dot(factors1, factors2)\n",
    "        norm = np.linalg.norm(factors1) * np.linalg.norm(factors2)\n",
    "        return dot / norm if norm > 0 else 0\n",
    "    \n",
    "    def rerank(self, user_id, n=10, candidate_size=100):\n",
    "        candidates = self.model.recommend(user_id, n=candidate_size, exclude_seen=True)\n",
    "        with torch.no_grad():\n",
    "            user_tensor = torch.LongTensor([user_id] * self.model.n_items).to(self.model.device)\n",
    "            item_tensor = torch.LongTensor(list(range(self.model.n_items))).to(self.model.device)\n",
    "            relevance_scores = self.model.model(user_tensor, item_tensor).cpu().numpy()\n",
    "        candidate_scores = relevance_scores[candidates]\n",
    "        min_score = np.min(candidate_scores)\n",
    "        max_score = np.max(candidate_scores)\n",
    "        score_range = max_score - min_score\n",
    "        normalized_scores = ((candidate_scores - min_score) / score_range) if score_range > 0 else np.zeros_like(candidate_scores)\n",
    "        \n",
    "        selected = []\n",
    "        if candidates.size > 0:\n",
    "            selected.append(candidates[np.argmax(normalized_scores)])\n",
    "            remaining = set(candidates) - set(selected)\n",
    "        else:\n",
    "            remaining = set()\n",
    "        while len(selected) < n and remaining:\n",
    "            max_mmr = -np.inf\n",
    "            max_item = None\n",
    "            for item in remaining:\n",
    "                idx = np.where(candidates == item)[0][0]\n",
    "                relevance = normalized_scores[idx]\n",
    "                max_sim = 0\n",
    "                for sel in selected:\n",
    "                    sim = self.calculate_item_similarity(item, sel)\n",
    "                    max_sim = max(max_sim, sim)\n",
    "                mmr_score = self.lambda_param * relevance - (1 - self.lambda_param) * max_sim\n",
    "                if mmr_score > max_mmr:\n",
    "                    max_mmr = mmr_score\n",
    "                    max_item = item\n",
    "            if max_item is None:\n",
    "                break\n",
    "            selected.append(max_item)\n",
    "            remaining.remove(max_item)\n",
    "        return np.array(selected)\n",
    "\n",
    "#################################\n",
    "# EVALUATION METRICS (same as before)\n",
    "#################################\n",
    "\n",
    "def calculate_ndcg(recommended_items, relevant_items, relevant_scores, k=None):\n",
    "    if k is None:\n",
    "        k = len(recommended_items)\n",
    "    else:\n",
    "        k = min(k, len(recommended_items))\n",
    "    \n",
    "    # Build relevance map (ensuring all scores are floats)\n",
    "    relevance_map = {}\n",
    "    for item_id, score in zip(relevant_items, relevant_scores):\n",
    "        try:\n",
    "            score_float = float(score)\n",
    "        except (ValueError, TypeError):\n",
    "            score_float = 0.0\n",
    "        score_float = min(score_float, 10.0)\n",
    "        relevance_map[item_id] = score_float\n",
    "    \n",
    "    # Compute DCG for recommended items\n",
    "    dcg = 0.0\n",
    "    for i, item_id in enumerate(recommended_items[:k]):\n",
    "        if item_id in relevance_map:\n",
    "            rel = float(relevance_map[item_id])\n",
    "            dcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Compute IDCG (ideal DCG)\n",
    "    sorted_relevant = []\n",
    "    for item_id, score in zip(relevant_items, relevant_scores):\n",
    "        try:\n",
    "            score_float = float(score)\n",
    "        except (ValueError, TypeError):\n",
    "            score_float = 0.0\n",
    "        score_float = min(score_float, 10.0)\n",
    "        sorted_relevant.append((item_id, score_float))\n",
    "    sorted_relevant.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    idcg = 0.0\n",
    "    for i, (item_id, rel) in enumerate(sorted_relevant[:k]):\n",
    "        idcg += (2 ** float(rel) - 1) / np.log2(i + 2)\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def calculate_precision(recommended_items, relevant_items):\n",
    "    num_relevant = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    return num_relevant / len(recommended_items) if recommended_items else 0\n",
    "\n",
    "def calculate_recall(recommended_items, relevant_items):\n",
    "    num_relevant = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    return num_relevant / len(relevant_items) if relevant_items else 0\n",
    "\n",
    "def calculate_diversity_metrics(recommendations, item_popularity, total_items, tail_items=None):\n",
    "    rec_counts = Counter(recommendations)\n",
    "    item_coverage = len(rec_counts) / total_items\n",
    "    sorted_counts = sorted(rec_counts.values())\n",
    "    n = len(sorted_counts)\n",
    "    if n == 0:\n",
    "        gini_index = 0\n",
    "    else:\n",
    "        cumulative_sum = sum((i + 1) * count for i, count in enumerate(sorted_counts))\n",
    "        gini_index = (2 * cumulative_sum) / (n * sum(sorted_counts)) - (n + 1) / n\n",
    "    rec_total = sum(rec_counts.values())\n",
    "    probabilities = [count / rec_total for count in rec_counts.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    max_entropy = np.log2(min(total_items, rec_total))\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    if tail_items is None:\n",
    "        sorted_pop_indices = np.argsort(item_popularity)\n",
    "        num_tail_items = int(len(sorted_pop_indices) * 0.2)\n",
    "        tail_items = set(sorted_pop_indices[:num_tail_items])\n",
    "    tail_recommendations = sum(1 for item in recommendations if item in tail_items)\n",
    "    tail_percentage = tail_recommendations / len(recommendations) if recommendations else 0\n",
    "    metrics = {\n",
    "        'item_coverage': item_coverage,\n",
    "        'gini_index': gini_index,\n",
    "        'shannon_entropy': normalized_entropy,\n",
    "        'tail_percentage': tail_percentage\n",
    "    }\n",
    "    return metrics, tail_items\n",
    "\n",
    "#################################\n",
    "# COMPREHENSIVE EVALUATION (NeuMF on LastFM)\n",
    "#################################\n",
    "\n",
    "def comprehensive_evaluation_multiple_rerankers(k=10, sample_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate NeuMF with diversity reranking on the LastFM dataset.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"COMPREHENSIVE EVALUATION WITH MULTIPLE RERANKERS (k={k})\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nLoading LastFM dataset...\")\n",
    "    ratings_df, dummy_df = load_lastfm()\n",
    "    \n",
    "    print(\"Splitting data for evaluation...\")\n",
    "    value_counts = ratings_df['user_id'].value_counts()\n",
    "    if value_counts.min() >= 2:\n",
    "        print(\"Using stratified sampling...\")\n",
    "        train_df, test_df = train_test_split(\n",
    "            ratings_df, test_size=0.2, stratify=ratings_df['user_id'], random_state=42\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using random sampling (some users have only 1 rating)...\")\n",
    "        train_df, test_df = train_test_split(\n",
    "            ratings_df, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    print(\"Creating user-item matrix...\")\n",
    "    user_item_matrix, user_mapping, item_mapping = create_user_item_matrix(train_df)\n",
    "    reverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "    reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    \n",
    "    test_relevant_items = defaultdict(list)\n",
    "    test_relevant_scores = defaultdict(list)\n",
    "    for _, row in test_df.iterrows():\n",
    "        uid, iid, rating = row['user_id'], row['item_id'], row['rating']\n",
    "        if uid in user_mapping and iid in item_mapping:\n",
    "            test_relevant_items[uid].append(iid)\n",
    "            test_relevant_scores[uid].append(rating)\n",
    "    \n",
    "    print(\"\\nTraining NeuMF model...\")\n",
    "    model = NeuMFRecommender(latent_dim=32, epochs=10, batch_size=256)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    print(\"\\nInitializing rerankers...\")\n",
    "    # Wir verwenden hier den SimpleRerankerNeuMF (du kannst auch den MMRRerankerNeuMF verwenden)\n",
    "    simple_reranker = SimpleRerankerNeuMF(model=model, alpha=0.7)\n",
    "    mmr_reranker = MMRRerankerNeuMF(model=model, lambda_param=0.7)\n",
    "    \n",
    "    rerankers = {\n",
    "        \"Original NeuMF\": None,\n",
    "        \"Simple Reranker\": simple_reranker,\n",
    "        \"MMR Reranker\": mmr_reranker\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    if sample_size is not None and sample_size < len(test_relevant_items):\n",
    "        eval_users = random.sample(list(test_relevant_items.keys()), sample_size)\n",
    "    else:\n",
    "        eval_users = list(test_relevant_items.keys())\n",
    "    print(f\"\\nEvaluating {len(eval_users)} users...\")\n",
    "    \n",
    "    for name, reranker in rerankers.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        ndcg_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        all_recs = []\n",
    "        for uid in eval_users:\n",
    "            if not test_relevant_items[uid]:\n",
    "                continue\n",
    "            user_idx = user_mapping[uid]\n",
    "            if reranker is None:\n",
    "                rec_idx = model.recommend(user_idx, n=k)\n",
    "            else:\n",
    "                rec_idx = reranker.rerank(user_idx, n=k)\n",
    "            rec = [reverse_item_mapping[idx] for idx in rec_idx]\n",
    "            all_recs.extend(rec_idx)\n",
    "            ndcg_scores.append(calculate_ndcg(rec, test_relevant_items[uid], test_relevant_scores[uid]))\n",
    "            precision_scores.append(calculate_precision(rec, test_relevant_items[uid]))\n",
    "            recall_scores.append(calculate_recall(rec, test_relevant_items[uid]))\n",
    "        accuracy_metrics = {\n",
    "            f'ndcg@{k}': np.mean(ndcg_scores),\n",
    "            f'precision@{k}': np.mean(precision_scores),\n",
    "            f'recall@{k}': np.mean(recall_scores)\n",
    "        }\n",
    "        item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    item_popularity[item] += 1\n",
    "        diversity_metrics, _ = calculate_diversity_metrics(\n",
    "            recommendations=all_recs,\n",
    "            item_popularity=item_popularity,\n",
    "            total_items=model.n_items\n",
    "        )\n",
    "        all_results[name] = {\n",
    "            'accuracy': accuracy_metrics,\n",
    "            'diversity': diversity_metrics\n",
    "        }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \" ACCURACY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for name in rerankers.keys():\n",
    "        print(f\"{name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    for metric in [f'ndcg@{k}', f'precision@{k}', f'recall@{k}']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original NeuMF\"]['accuracy'][metric]\n",
    "        for name in rerankers.keys():\n",
    "            value = all_results[name]['accuracy'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            if name == \"Original NeuMF\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \" DIVERSITY METRICS COMPARISON \" + \"=\"*30)\n",
    "    print(f\"{'Metric':<15}\", end='')\n",
    "    for name in rerankers.keys():\n",
    "        print(f\"{name:<20}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    for metric in ['item_coverage', 'gini_index', 'shannon_entropy', 'tail_percentage']:\n",
    "        print(f\"{metric:<15}\", end='')\n",
    "        baseline = all_results[\"Original NeuMF\"]['diversity'][metric]\n",
    "        for name in rerankers.keys():\n",
    "            value = all_results[name]['diversity'][metric]\n",
    "            change = ((value - baseline) / baseline * 100) if baseline > 0 else float('inf')\n",
    "            if name == \"Original NeuMF\":\n",
    "                print(f\"{value:.4f}{' '*15}\", end='')\n",
    "            else:\n",
    "                print(f\"{value:.4f} ({change:+.1f}%){' '*5}\", end='')\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \" METRIC INTERPRETATIONS \" + \"=\"*30)\n",
    "    print(\"Accuracy Metrics:\")\n",
    "    print(\"- NDCG: Higher is better, measures ranking quality\")\n",
    "    print(\"- Precision: Higher is better, measures relevant item ratio\")\n",
    "    print(\"- Recall: Higher is better, measures coverage of relevant items\")\n",
    "    print(\"\\nDiversity Metrics:\")\n",
    "    print(\"- Item Coverage: Higher means more catalog items are recommended\")\n",
    "    print(\"- Gini Index: Lower indicates more equality in recommendations\")\n",
    "    print(\"- Shannon Entropy: Higher means more diverse recommendations\")\n",
    "    print(\"- Tail Percentage: Higher means more niche items are recommended\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Execute evaluation when run directly\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_evaluation_multiple_rerankers(k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3556666-9d37-4426-ae1b-059b383443f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
