{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "238190fb-8a13-460e-9dea-2e406387286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MovieLens 100K dataset...\n",
      "Splitting data for evaluation...\n",
      "Creating user-item matrix...\n",
      "Training BPR model...\n",
      "Completed iteration 10/30\n",
      "Completed iteration 20/30\n",
      "Completed iteration 30/30\n",
      "\n",
      "Evaluating original BPR recommendations...\n",
      "NDCG@10: 0.2808\n",
      "Precision@10: 0.3031\n",
      "Recall@10: 0.1953\n",
      "Users evaluated: 943\n",
      "\n",
      "Initializing reranker...\n",
      "\n",
      "Evaluating reranked recommendations...\n",
      "NDCG@10: 0.2786\n",
      "Precision@10: 0.3043\n",
      "Recall@10: 0.1946\n",
      "Users evaluated: 943\n",
      "\n",
      "Metrics Comparison:\n",
      "Metric          Original        Reranked        Change (%)     \n",
      "------------------------------------------------------------\n",
      "ndcg@10         0.2808           0.2786           -0.78%\n",
      "precision@10    0.3031           0.3043           +0.42%\n",
      "recall@10       0.1953           0.1946           -0.35%\n"
     ]
    }
   ],
   "source": [
    "# Simple BPR Reranking with Accuracy Metrics Only\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluate_bpr_accuracy(model, user_mapping, item_mapping, test_df, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate BPR model accuracy metrics only\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained BPR model\n",
    "    - user_mapping: mapping from original user IDs to matrix indices\n",
    "    - item_mapping: mapping from original item IDs to matrix indices\n",
    "    - test_df: test set DataFrame\n",
    "    - k: number of recommendations to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: dictionary with accuracy metrics\n",
    "    \"\"\"\n",
    "    reverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "    reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    \n",
    "    # Create test set ground truth\n",
    "    test_relevant_items = defaultdict(list)\n",
    "    test_relevant_scores = defaultdict(list)\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Only include users and items that exist in our mappings\n",
    "        if user_id in user_mapping and item_id in item_mapping:\n",
    "            test_relevant_items[user_id].append(item_id)\n",
    "            test_relevant_scores[user_id].append(rating)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    ndcg_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    # Evaluate for each user in test set\n",
    "    for user_id in test_relevant_items:\n",
    "        # Skip if user has no relevant items\n",
    "        if not test_relevant_items[user_id]:\n",
    "            continue\n",
    "        \n",
    "        # Get model predictions\n",
    "        user_idx = user_mapping[user_id]\n",
    "        recommended_items_idx = model.recommend(user_idx, n=k)\n",
    "        recommended_items = [reverse_item_mapping[idx] for idx in recommended_items_idx]\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        ndcg = calculate_ndcg(recommended_items, \n",
    "                             test_relevant_items[user_id],\n",
    "                             test_relevant_scores[user_id])\n",
    "        ndcg_scores.append(ndcg)\n",
    "        \n",
    "        # Calculate Precision\n",
    "        precision = calculate_precision(recommended_items, test_relevant_items[user_id])\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        # Calculate Recall\n",
    "        recall = calculate_recall(recommended_items, test_relevant_items[user_id])\n",
    "        recall_scores.append(recall)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0\n",
    "    avg_precision = np.mean(precision_scores) if precision_scores else 0\n",
    "    avg_recall = np.mean(recall_scores) if recall_scores else 0\n",
    "    \n",
    "    print(f\"NDCG@{k}: {avg_ndcg:.4f}\")\n",
    "    print(f\"Precision@{k}: {avg_precision:.4f}\")\n",
    "    print(f\"Recall@{k}: {avg_recall:.4f}\")\n",
    "    print(f\"Users evaluated: {len(ndcg_scores)}\")\n",
    "    \n",
    "    metrics = {\n",
    "        f'ndcg@{k}': avg_ndcg,\n",
    "        f'precision@{k}': avg_precision,\n",
    "        f'recall@{k}': avg_recall\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_ndcg(recommended_items, relevant_items, relevant_scores, k=None):\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain\n",
    "    \n",
    "    Parameters:\n",
    "    - recommended_items: list of recommended item IDs\n",
    "    - relevant_items: list of ground truth relevant item IDs\n",
    "    - relevant_scores: list of relevance scores (ratings) for the relevant items\n",
    "    - k: cutoff for calculation (use all items if None)\n",
    "    \n",
    "    Returns:\n",
    "    - ndcg: NDCG score\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(recommended_items)\n",
    "    else:\n",
    "        k = min(k, len(recommended_items))\n",
    "    \n",
    "    # Create a dictionary mapping relevant items to their scores\n",
    "    relevance_map = {item_id: score for item_id, score in zip(relevant_items, relevant_scores)}\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = 0\n",
    "    for i, item_id in enumerate(recommended_items[:k]):\n",
    "        if item_id in relevance_map:\n",
    "            # Use rating as relevance score\n",
    "            rel = relevance_map[item_id]\n",
    "            # DCG formula: (2^rel - 1) / log2(i+2)\n",
    "            dcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Calculate ideal DCG (IDCG)\n",
    "    # Sort relevant items by their relevance scores in descending order\n",
    "    sorted_relevant = sorted(zip(relevant_items, relevant_scores), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    idcg = 0\n",
    "    for i, (item_id, rel) in enumerate(sorted_relevant[:k]):\n",
    "        # IDCG formula: (2^rel - 1) / log2(i+2)\n",
    "        idcg += (2 ** rel - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate NDCG\n",
    "    ndcg = dcg / idcg\n",
    "    \n",
    "    return ndcg\n",
    "\n",
    "def calculate_precision(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Precision@k\n",
    "    \n",
    "    Parameters:\n",
    "    - recommended_items: list of recommended item IDs\n",
    "    - relevant_items: list of ground truth relevant item IDs\n",
    "    \n",
    "    Returns:\n",
    "    - precision: Precision@k score\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = num_relevant_recommended / len(recommended_items) if recommended_items else 0\n",
    "    \n",
    "    return precision\n",
    "\n",
    "def calculate_recall(recommended_items, relevant_items):\n",
    "    \"\"\"\n",
    "    Calculate Recall@k\n",
    "    \n",
    "    Parameters:\n",
    "    - recommended_items: list of recommended item IDs\n",
    "    - relevant_items: list of ground truth relevant item IDs\n",
    "    \n",
    "    Returns:\n",
    "    - recall: Recall@k score\n",
    "    \"\"\"\n",
    "    # Count number of relevant items in recommended items\n",
    "    num_relevant_recommended = sum(1 for item in recommended_items if item in relevant_items)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = num_relevant_recommended / len(relevant_items) if relevant_items else 0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "class SimpleReranker:\n",
    "    \"\"\"\n",
    "    Simple reranker that balances original scores with diversity\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Initialize reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained BPR model\n",
    "        - alpha: weight for original scores (between 0 and 1)\n",
    "                 higher alpha means more focus on accuracy\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Calculate item popularity\n",
    "        self.item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    self.item_popularity[item] += 1\n",
    "        \n",
    "        # Normalize popularity\n",
    "        max_pop = np.max(self.item_popularity)\n",
    "        if max_pop > 0:\n",
    "            self.norm_popularity = self.item_popularity / max_pop\n",
    "        else:\n",
    "            self.norm_popularity = np.zeros_like(self.item_popularity)\n",
    "    \n",
    "    def rerank(self, user_id, n=10):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index in the model\n",
    "        - n: number of recommendations to return\n",
    "        \n",
    "        Returns:\n",
    "        - reranked_items: list of reranked item indices\n",
    "        \"\"\"\n",
    "        # Get original scores for all items\n",
    "        original_scores = np.dot(self.model.user_factors[user_id], self.model.item_factors.T)\n",
    "        \n",
    "        # Exclude seen items\n",
    "        if user_id in self.model.user_items:\n",
    "            seen_items = list(self.model.user_items[user_id])\n",
    "            original_scores[seen_items] = -np.inf\n",
    "        \n",
    "        # Get candidate items (top k*3)\n",
    "        candidates = np.argsort(original_scores)[::-1][:n*3]\n",
    "        \n",
    "        # Initialize selected items and scores\n",
    "        selected = []\n",
    "        \n",
    "        # Iteratively select items\n",
    "        while len(selected) < n:\n",
    "            best_score = -np.inf\n",
    "            best_item = None\n",
    "            \n",
    "            for item in candidates:\n",
    "                if item in selected:\n",
    "                    continue\n",
    "                \n",
    "                # Original score component (normalized)\n",
    "                score_orig = original_scores[item]\n",
    "                \n",
    "                # Diversity component\n",
    "                diversity_score = 0\n",
    "                if selected:\n",
    "                    # Use item factors to calculate similarity\n",
    "                    item_factors = self.model.item_factors[item]\n",
    "                    selected_factors = self.model.item_factors[selected]\n",
    "                    \n",
    "                    # Calculate average similarity\n",
    "                    similarities = []\n",
    "                    for i, sel_factors in enumerate(selected_factors):\n",
    "                        # Cosine similarity\n",
    "                        dot_product = np.dot(item_factors, sel_factors)\n",
    "                        norm_product = np.linalg.norm(item_factors) * np.linalg.norm(sel_factors)\n",
    "                        \n",
    "                        if norm_product > 0:\n",
    "                            sim = dot_product / norm_product\n",
    "                            similarities.append(sim)\n",
    "                    \n",
    "                    if similarities:\n",
    "                        avg_sim = np.mean(similarities)\n",
    "                        diversity_score = 1 - avg_sim\n",
    "                \n",
    "                # Novelty component (inverse popularity)\n",
    "                novelty_score = 1 - self.norm_popularity[item]\n",
    "                \n",
    "                # Calculate weighted score\n",
    "                combined_score = (\n",
    "                    self.alpha * score_orig + \n",
    "                    (1 - self.alpha) * 0.5 * diversity_score + \n",
    "                    (1 - self.alpha) * 0.5 * novelty_score\n",
    "                )\n",
    "                \n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_item = item\n",
    "            \n",
    "            if best_item is None:\n",
    "                break\n",
    "                \n",
    "            selected.append(best_item)\n",
    "            \n",
    "        return selected\n",
    "\n",
    "def simple_reranking_evaluation():\n",
    "    \"\"\"\n",
    "    Run a simple evaluation of original BPR vs reranked recommendations\n",
    "    \"\"\"\n",
    "    print(\"Loading MovieLens 100K dataset...\")\n",
    "    ratings_df, movie_df = load_movielens_100k()\n",
    "    \n",
    "    print(\"Splitting data for evaluation...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        ratings_df, \n",
    "        test_size=0.2, \n",
    "        stratify=ratings_df['user_id'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Creating user-item matrix...\")\n",
    "    user_item_matrix, user_mapping, item_mapping = create_user_item_matrix(train_df)\n",
    "    \n",
    "    print(\"Training BPR model...\")\n",
    "    model = BPRRecommender(factors=50, learning_rate=0.01, \n",
    "                          regularization=0.01, iterations=30)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    print(\"\\nEvaluating original BPR recommendations...\")\n",
    "    original_metrics = evaluate_bpr_accuracy(\n",
    "        model=model,\n",
    "        user_mapping=user_mapping,\n",
    "        item_mapping=item_mapping,\n",
    "        test_df=test_df,\n",
    "        k=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\nInitializing reranker...\")\n",
    "    reranker = SimpleReranker(model=model, alpha=0.7)\n",
    "    \n",
    "    print(\"\\nEvaluating reranked recommendations...\")\n",
    "    # Initialize metrics\n",
    "    ndcg_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    # Create test set ground truth\n",
    "    test_relevant_items = defaultdict(list)\n",
    "    test_relevant_scores = defaultdict(list)\n",
    "    \n",
    "    reverse_user_mapping = {v: k for k, v in user_mapping.items()}\n",
    "    reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "    \n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        item_id = row['item_id']\n",
    "        rating = row['rating']\n",
    "        \n",
    "        # Only include users and items that exist in our mappings\n",
    "        if user_id in user_mapping and item_id in item_mapping:\n",
    "            test_relevant_items[user_id].append(item_id)\n",
    "            test_relevant_scores[user_id].append(rating)\n",
    "    \n",
    "    # Evaluate reranking for each user\n",
    "    for user_id in test_relevant_items:\n",
    "        # Skip if user has no relevant items\n",
    "        if not test_relevant_items[user_id]:\n",
    "            continue\n",
    "        \n",
    "        # Get user index\n",
    "        user_idx = user_mapping[user_id]\n",
    "        \n",
    "        # Get reranked recommendations\n",
    "        reranked_items_idx = reranker.rerank(user_idx, n=10)\n",
    "        reranked_items = [reverse_item_mapping[idx] for idx in reranked_items_idx]\n",
    "        \n",
    "        # Calculate NDCG\n",
    "        ndcg = calculate_ndcg(reranked_items, \n",
    "                            test_relevant_items[user_id],\n",
    "                            test_relevant_scores[user_id])\n",
    "        ndcg_scores.append(ndcg)\n",
    "        \n",
    "        # Calculate Precision\n",
    "        precision = calculate_precision(reranked_items, test_relevant_items[user_id])\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        # Calculate Recall\n",
    "        recall = calculate_recall(reranked_items, test_relevant_items[user_id])\n",
    "        recall_scores.append(recall)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0\n",
    "    avg_precision = np.mean(precision_scores) if precision_scores else 0\n",
    "    avg_recall = np.mean(recall_scores) if recall_scores else 0\n",
    "    \n",
    "    reranked_metrics = {\n",
    "        'ndcg@10': avg_ndcg,\n",
    "        'precision@10': avg_precision,\n",
    "        'recall@10': avg_recall\n",
    "    }\n",
    "    \n",
    "    print(f\"NDCG@10: {avg_ndcg:.4f}\")\n",
    "    print(f\"Precision@10: {avg_precision:.4f}\")\n",
    "    print(f\"Recall@10: {avg_recall:.4f}\")\n",
    "    print(f\"Users evaluated: {len(ndcg_scores)}\")\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nMetrics Comparison:\")\n",
    "    print(f\"{'Metric':<15} {'Original':<15} {'Reranked':<15} {'Change (%)':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for metric in ['ndcg@10', 'precision@10', 'recall@10']:\n",
    "        orig = original_metrics[metric]\n",
    "        rerank = reranked_metrics[metric]\n",
    "        change = ((rerank - orig) / orig) * 100 if orig > 0 else float('inf')\n",
    "        print(f\"{metric:<15} {orig:.4f}{' '*10} {rerank:.4f}{' '*10} {change:+.2f}%\")\n",
    "\n",
    "# Include the BPR implementation directly instead of importing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class BPRRecommender:\n",
    "    def __init__(self, factors=50, learning_rate=0.01, regularization=0.01, iterations=50, random_state=42):\n",
    "        \"\"\"\n",
    "        Bayesian Personalized Ranking (BPR) recommender algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        - factors: dimensionality of latent factors\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - regularization: regularization term to prevent overfitting\n",
    "        - iterations: number of training iterations\n",
    "        - random_state: random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.factors = factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Train the BPR model on the user-item matrix\n",
    "        \n",
    "        Parameters:\n",
    "        - user_item_matrix: scipy sparse matrix with user-item interactions\n",
    "        \n",
    "        Returns:\n",
    "        - self\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.n_users, self.n_items = user_item_matrix.shape\n",
    "        \n",
    "        # Initialize latent factors\n",
    "        self.user_factors = np.random.normal(0, 0.1, (self.n_users, self.factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (self.n_items, self.factors))\n",
    "        \n",
    "        # Create a dictionary of items each user has interacted with\n",
    "        self.user_items = defaultdict(set)\n",
    "        for user, item in zip(*self.user_item_matrix.nonzero()):\n",
    "            self.user_items[user].add(item)\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.iterations):\n",
    "            # Sample triplets for training\n",
    "            for _ in range(user_item_matrix.nnz):\n",
    "                user, pos_item, neg_item = self._sample_triplet()\n",
    "                self._update_factors(user, pos_item, neg_item)\n",
    "            \n",
    "            # Print progress\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                print(f\"Completed iteration {iteration + 1}/{self.iterations}\")\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def _sample_triplet(self):\n",
    "        \"\"\"\n",
    "        Sample a (user, positive_item, negative_item) triplet for training\n",
    "        \n",
    "        Returns:\n",
    "        - user: sampled user index\n",
    "        - pos_item: a positive item (one the user has interacted with)\n",
    "        - neg_item: a negative item (one the user has not interacted with)\n",
    "        \"\"\"\n",
    "        # Sample a user who has rated at least one item\n",
    "        user = random.choice(list(self.user_items.keys()))\n",
    "        \n",
    "        # Sample a positive item (one the user has interacted with)\n",
    "        pos_item = random.choice(list(self.user_items[user]))\n",
    "        \n",
    "        # Sample a negative item (one the user has not interacted with)\n",
    "        neg_item = random.randint(0, self.n_items - 1)\n",
    "        while neg_item in self.user_items[user]:\n",
    "            neg_item = random.randint(0, self.n_items - 1)\n",
    "            \n",
    "        return user, pos_item, neg_item\n",
    "    \n",
    "    def _update_factors(self, user, pos_item, neg_item):\n",
    "        \"\"\"\n",
    "        Update model parameters based on a triplet\n",
    "        \n",
    "        Parameters:\n",
    "        - user: user index\n",
    "        - pos_item: positive item index\n",
    "        - neg_item: negative item index\n",
    "        \"\"\"\n",
    "        # Calculate prediction for positive and negative items\n",
    "        pos_pred = np.dot(self.user_factors[user], self.item_factors[pos_item])\n",
    "        neg_pred = np.dot(self.user_factors[user], self.item_factors[neg_item])\n",
    "        \n",
    "        # Calculate prediction difference\n",
    "        diff = neg_pred - pos_pred\n",
    "        \n",
    "        # Calculate sigmoid gradient\n",
    "        sigmoid = 1.0 / (1.0 + np.exp(-diff))\n",
    "        \n",
    "        # Calculate gradients\n",
    "        grad_user = sigmoid * (self.item_factors[neg_item] - self.item_factors[pos_item]) + self.regularization * self.user_factors[user]\n",
    "        grad_pos_item = sigmoid * (-self.user_factors[user]) + self.regularization * self.item_factors[pos_item]\n",
    "        grad_neg_item = sigmoid * self.user_factors[user] + self.regularization * self.item_factors[neg_item]\n",
    "        \n",
    "        # Update factors\n",
    "        self.user_factors[user] -= self.learning_rate * grad_user\n",
    "        self.item_factors[pos_item] -= self.learning_rate * grad_pos_item\n",
    "        self.item_factors[neg_item] -= self.learning_rate * grad_neg_item\n",
    "    \n",
    "    def recommend(self, user_id, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate item recommendations for a user\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index\n",
    "        - n: number of recommendations to generate\n",
    "        - exclude_seen: whether to exclude items the user has already interacted with\n",
    "        \n",
    "        Returns:\n",
    "        - list of n recommended item indices\n",
    "        \"\"\"\n",
    "        # Calculate predicted scores for all items\n",
    "        scores = np.dot(self.user_factors[user_id], self.item_factors.T)\n",
    "        \n",
    "        # If requested, exclude items the user has already interacted with\n",
    "        if exclude_seen and user_id in self.user_items:\n",
    "            seen_items = list(self.user_items[user_id])\n",
    "            scores[seen_items] = -np.inf\n",
    "            \n",
    "        # Get top n items by score\n",
    "        top_items = np.argsort(scores)[::-1][:n]\n",
    "        \n",
    "        return top_items\n",
    "    \n",
    "    def get_similar_items(self, item_id, n=10):\n",
    "        \"\"\"\n",
    "        Find items similar to a given item based on latent factors\n",
    "        \n",
    "        Parameters:\n",
    "        - item_id: item index\n",
    "        - n: number of similar items to retrieve\n",
    "        \n",
    "        Returns:\n",
    "        - list of n similar item indices\n",
    "        \"\"\"\n",
    "        # Calculate cosine similarity between the item and all other items\n",
    "        sim_scores = cosine_similarity([self.item_factors[item_id]], self.item_factors)[0]\n",
    "        \n",
    "        # Exclude the item itself\n",
    "        sim_scores[item_id] = -np.inf\n",
    "        \n",
    "        # Get top n items by similarity\n",
    "        similar_items = np.argsort(sim_scores)[::-1][:n]\n",
    "        \n",
    "        return similar_items\n",
    "\n",
    "\n",
    "def load_movielens_100k(path=\"ml-100k\"):\n",
    "    \"\"\"\n",
    "    Load the MovieLens 100K dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - path: path to the ML-100K dataset directory\n",
    "    \n",
    "    Returns:\n",
    "    - ratings_df: pandas DataFrame with columns 'user_id', 'item_id', 'rating'\n",
    "    - movie_df: pandas DataFrame with movie information\n",
    "    \"\"\"\n",
    "    # Load ratings\n",
    "    ratings_df = pd.read_csv(f\"{path}/u.data\", sep='\\t', \n",
    "                           names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Load movie information\n",
    "    movie_df = pd.read_csv(f\"{path}/u.item\", sep='|', encoding='latin-1',\n",
    "                          names=['item_id', 'title', 'release_date', 'video_release_date',\n",
    "                                 'IMDb_URL'] + [f'genre_{i}' for i in range(19)])\n",
    "    \n",
    "    return ratings_df, movie_df\n",
    "\n",
    "\n",
    "def create_user_item_matrix(ratings_df):\n",
    "    \"\"\"\n",
    "    Create a sparse user-item interaction matrix from ratings\n",
    "    \n",
    "    Parameters:\n",
    "    - ratings_df: pandas DataFrame with columns 'user_id', 'item_id', 'rating'\n",
    "    \n",
    "    Returns:\n",
    "    - user_item_matrix: scipy sparse matrix with user-item interactions\n",
    "    - user_mapping: dict mapping original user IDs to matrix indices\n",
    "    - item_mapping: dict mapping original item IDs to matrix indices\n",
    "    \"\"\"\n",
    "    # Create mappings from original IDs to matrix indices\n",
    "    user_ids = ratings_df['user_id'].unique()\n",
    "    item_ids = ratings_df['item_id'].unique()\n",
    "    \n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(user_ids)}\n",
    "    item_mapping = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "    \n",
    "    # Map original IDs to matrix indices\n",
    "    rows = ratings_df['user_id'].map(user_mapping)\n",
    "    cols = ratings_df['item_id'].map(item_mapping)\n",
    "    \n",
    "    # Create binary matrix (1 if interaction exists, 0 otherwise)\n",
    "    data = np.ones(len(ratings_df))\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                 shape=(len(user_mapping), len(item_mapping)))\n",
    "    \n",
    "    return user_item_matrix, user_mapping, item_mapping\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simple_reranking_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f1f0b5-774b-448e-85b1-74a28a76833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MovieLens 100K dataset...\n",
      "Splitting data for evaluation...\n",
      "Creating user-item matrix...\n",
      "Training BPR model...\n",
      "Completed iteration 10/30\n",
      "Completed iteration 20/30\n",
      "Completed iteration 30/30\n",
      "\n",
      "Generating original BPR recommendations for all users...\n",
      "\n",
      "Evaluating diversity metrics for original BPR recommendations...\n",
      "Item Coverage: 0.1552 (257 out of 1656 items)\n",
      "Gini Index: 0.6976 (0 is perfect equality, 1 is perfect inequality)\n",
      "Shannon Entropy: 0.6279 (normalized, higher is more diverse)\n",
      "Tail Percentage: 0.0000 (0 out of 9430 recommendations)\n",
      "\n",
      "Initializing reranker...\n",
      "\n",
      "Generating reranked recommendations for all users...\n",
      "\n",
      "Evaluating diversity metrics for reranked recommendations...\n",
      "Item Coverage: 0.1697 (281 out of 1656 items)\n",
      "Gini Index: 0.7007 (0 is perfect equality, 1 is perfect inequality)\n",
      "Shannon Entropy: 0.6386 (normalized, higher is more diverse)\n",
      "Tail Percentage: 0.0000 (0 out of 9430 recommendations)\n",
      "\n",
      "Diversity Metrics Comparison:\n",
      "Metric               Original        Reranked        Change (%)     \n",
      "-----------------------------------------------------------------\n",
      "item_coverage        0.1552           0.1697           +9.34%\n",
      "gini_index           0.6976           0.7007           +0.44%\n",
      "shannon_entropy      0.6279           0.6386           +1.70%\n",
      "tail_percentage      0.0000           0.0000           +inf%\n"
     ]
    }
   ],
   "source": [
    "# BPR Reranking with Diversity Metrics Only\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def evaluate_diversity_metrics(model, recommendations, total_items, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate diversity metrics for a set of recommendations\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained BPR model\n",
    "    - recommendations: list of recommended item indices\n",
    "    - total_items: total number of items in the catalog\n",
    "    - k: number of recommendations per user\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: dictionary with diversity metrics\n",
    "    \"\"\"\n",
    "    # Count recommendations for each item\n",
    "    rec_counts = Counter(recommendations)\n",
    "    \n",
    "    # Calculate item popularity from the training data\n",
    "    item_popularity = np.zeros(model.n_items)\n",
    "    for user in range(model.n_users):\n",
    "        if user in model.user_items:\n",
    "            for item in model.user_items[user]:\n",
    "                item_popularity[item] += 1\n",
    "    \n",
    "    # Sort items by popularity for tail calculation\n",
    "    sorted_pop_indices = np.argsort(item_popularity)\n",
    "    num_tail_items = int(len(sorted_pop_indices) * 0.2)  # 20% least popular items\n",
    "    tail_items = set(sorted_pop_indices[:num_tail_items])\n",
    "    \n",
    "    # 1. Item Coverage\n",
    "    recommended_items = len(rec_counts)\n",
    "    item_coverage = recommended_items / total_items\n",
    "    \n",
    "    # 2. Gini Index\n",
    "    sorted_counts = sorted(rec_counts.values())\n",
    "    n = len(sorted_counts)\n",
    "    \n",
    "    if n == 0:\n",
    "        gini_index = 0\n",
    "    else:\n",
    "        cumulative_sum = 0\n",
    "        for i, count in enumerate(sorted_counts):\n",
    "            cumulative_sum += (i + 1) * count\n",
    "        \n",
    "        # Gini index formula\n",
    "        gini_index = (2 * cumulative_sum) / (n * sum(sorted_counts)) - (n + 1) / n\n",
    "    \n",
    "    # 3. Shannon Entropy\n",
    "    recommendations_count = sum(rec_counts.values())\n",
    "    probabilities = [count / recommendations_count for count in rec_counts.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "    \n",
    "    # Normalize entropy\n",
    "    max_entropy = np.log2(min(total_items, recommendations_count))\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    \n",
    "    # 4. Tail Percentage\n",
    "    tail_recommendations = sum(1 for item in recommendations if item in tail_items)\n",
    "    tail_percentage = tail_recommendations / len(recommendations) if recommendations else 0\n",
    "    \n",
    "    # Create results dictionary\n",
    "    metrics = {\n",
    "        'item_coverage': item_coverage,\n",
    "        'gini_index': gini_index,\n",
    "        'shannon_entropy': normalized_entropy,\n",
    "        'tail_percentage': tail_percentage\n",
    "    }\n",
    "    \n",
    "    print(f\"Item Coverage: {item_coverage:.4f} ({recommended_items} out of {total_items} items)\")\n",
    "    print(f\"Gini Index: {gini_index:.4f} (0 is perfect equality, 1 is perfect inequality)\")\n",
    "    print(f\"Shannon Entropy: {normalized_entropy:.4f} (normalized, higher is more diverse)\")\n",
    "    print(f\"Tail Percentage: {tail_percentage:.4f} ({tail_recommendations} out of {len(recommendations)} recommendations)\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "class SimpleReranker:\n",
    "    \"\"\"\n",
    "    Simple reranker that balances original scores with diversity\n",
    "    \"\"\"\n",
    "    def __init__(self, model, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Initialize reranker\n",
    "        \n",
    "        Parameters:\n",
    "        - model: trained BPR model\n",
    "        - alpha: weight for original scores (between 0 and 1)\n",
    "                 higher alpha means more focus on accuracy\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Calculate item popularity\n",
    "        self.item_popularity = np.zeros(model.n_items)\n",
    "        for user in range(model.n_users):\n",
    "            if user in model.user_items:\n",
    "                for item in model.user_items[user]:\n",
    "                    self.item_popularity[item] += 1\n",
    "        \n",
    "        # Normalize popularity\n",
    "        max_pop = np.max(self.item_popularity)\n",
    "        if max_pop > 0:\n",
    "            self.norm_popularity = self.item_popularity / max_pop\n",
    "        else:\n",
    "            self.norm_popularity = np.zeros_like(self.item_popularity)\n",
    "    \n",
    "    def rerank(self, user_id, n=10):\n",
    "        \"\"\"\n",
    "        Generate reranked recommendations\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index in the model\n",
    "        - n: number of recommendations to return\n",
    "        \n",
    "        Returns:\n",
    "        - reranked_items: list of reranked item indices\n",
    "        \"\"\"\n",
    "        # Get original scores for all items\n",
    "        original_scores = np.dot(self.model.user_factors[user_id], self.model.item_factors.T)\n",
    "        \n",
    "        # Exclude seen items\n",
    "        if user_id in self.model.user_items:\n",
    "            seen_items = list(self.model.user_items[user_id])\n",
    "            original_scores[seen_items] = -np.inf\n",
    "        \n",
    "        # Get candidate items (top k*3)\n",
    "        candidates = np.argsort(original_scores)[::-1][:n*3]\n",
    "        \n",
    "        # Initialize selected items and scores\n",
    "        selected = []\n",
    "        \n",
    "        # Iteratively select items\n",
    "        while len(selected) < n:\n",
    "            best_score = -np.inf\n",
    "            best_item = None\n",
    "            \n",
    "            for item in candidates:\n",
    "                if item in selected:\n",
    "                    continue\n",
    "                \n",
    "                # Original score component (normalized)\n",
    "                score_orig = original_scores[item]\n",
    "                \n",
    "                # Diversity component\n",
    "                diversity_score = 0\n",
    "                if selected:\n",
    "                    # Use item factors to calculate similarity\n",
    "                    item_factors = self.model.item_factors[item]\n",
    "                    selected_factors = self.model.item_factors[selected]\n",
    "                    \n",
    "                    # Calculate average similarity\n",
    "                    similarities = []\n",
    "                    for i, sel_factors in enumerate(selected_factors):\n",
    "                        # Cosine similarity\n",
    "                        dot_product = np.dot(item_factors, sel_factors)\n",
    "                        norm_product = np.linalg.norm(item_factors) * np.linalg.norm(sel_factors)\n",
    "                        \n",
    "                        if norm_product > 0:\n",
    "                            sim = dot_product / norm_product\n",
    "                            similarities.append(sim)\n",
    "                    \n",
    "                    if similarities:\n",
    "                        avg_sim = np.mean(similarities)\n",
    "                        diversity_score = 1 - avg_sim\n",
    "                \n",
    "                # Novelty component (inverse popularity)\n",
    "                novelty_score = 1 - self.norm_popularity[item]\n",
    "                \n",
    "                # Calculate weighted score\n",
    "                combined_score = (\n",
    "                    self.alpha * score_orig + \n",
    "                    (1 - self.alpha) * 0.5 * diversity_score + \n",
    "                    (1 - self.alpha) * 0.5 * novelty_score\n",
    "                )\n",
    "                \n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_item = item\n",
    "            \n",
    "            if best_item is None:\n",
    "                break\n",
    "                \n",
    "            selected.append(best_item)\n",
    "            \n",
    "        return selected\n",
    "\n",
    "def diversity_reranking_evaluation():\n",
    "    \"\"\"\n",
    "    Run a simple evaluation of original BPR vs reranked recommendations using diversity metrics\n",
    "    \"\"\"\n",
    "    print(\"Loading MovieLens 100K dataset...\")\n",
    "    ratings_df, movie_df = load_movielens_100k()\n",
    "    \n",
    "    print(\"Splitting data for evaluation...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        ratings_df, \n",
    "        test_size=0.2, \n",
    "        stratify=ratings_df['user_id'], \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"Creating user-item matrix...\")\n",
    "    user_item_matrix, user_mapping, item_mapping = create_user_item_matrix(train_df)\n",
    "    \n",
    "    print(\"Training BPR model...\")\n",
    "    model = BPRRecommender(factors=50, learning_rate=0.01, \n",
    "                          regularization=0.01, iterations=30)\n",
    "    model.fit(user_item_matrix)\n",
    "    \n",
    "    print(\"\\nGenerating original BPR recommendations for all users...\")\n",
    "    all_original_recs = []\n",
    "    # Get recommendations for a sample of users (or all users if feasible)\n",
    "    num_users = model.n_users  # Use all users\n",
    "    \n",
    "    for user_idx in range(num_users):\n",
    "        # Skip users with no interactions\n",
    "        if user_idx not in model.user_items:\n",
    "            continue\n",
    "        # Get recommendations\n",
    "        recs = model.recommend(user_idx, n=10)\n",
    "        all_original_recs.extend(recs)\n",
    "    \n",
    "    print(\"\\nEvaluating diversity metrics for original BPR recommendations...\")\n",
    "    original_metrics = evaluate_diversity_metrics(\n",
    "        model=model,\n",
    "        recommendations=all_original_recs,\n",
    "        total_items=model.n_items\n",
    "    )\n",
    "    \n",
    "    print(\"\\nInitializing reranker...\")\n",
    "    reranker = SimpleReranker(model=model, alpha=0.7)\n",
    "    \n",
    "    print(\"\\nGenerating reranked recommendations for all users...\")\n",
    "    all_reranked_recs = []\n",
    "    \n",
    "    for user_idx in range(num_users):\n",
    "        # Skip users with no interactions\n",
    "        if user_idx not in model.user_items:\n",
    "            continue\n",
    "        # Get reranked recommendations\n",
    "        recs = reranker.rerank(user_idx, n=10)\n",
    "        all_reranked_recs.extend(recs)\n",
    "    \n",
    "    print(\"\\nEvaluating diversity metrics for reranked recommendations...\")\n",
    "    reranked_metrics = evaluate_diversity_metrics(\n",
    "        model=model,\n",
    "        recommendations=all_reranked_recs,\n",
    "        total_items=model.n_items\n",
    "    )\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nDiversity Metrics Comparison:\")\n",
    "    print(f\"{'Metric':<20} {'Original':<15} {'Reranked':<15} {'Change (%)':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for metric in ['item_coverage', 'gini_index', 'shannon_entropy', 'tail_percentage']:\n",
    "        orig = original_metrics[metric]\n",
    "        rerank = reranked_metrics[metric]\n",
    "        change = ((rerank - orig) / orig) * 100 if orig > 0 else float('inf')\n",
    "        print(f\"{metric:<20} {orig:.4f}{' '*10} {rerank:.4f}{' '*10} {change:+.2f}%\")\n",
    "\n",
    "# BPR Implementation\n",
    "class BPRRecommender:\n",
    "    def __init__(self, factors=50, learning_rate=0.01, regularization=0.01, iterations=50, random_state=42):\n",
    "        \"\"\"\n",
    "        Bayesian Personalized Ranking (BPR) recommender algorithm\n",
    "        \n",
    "        Parameters:\n",
    "        - factors: dimensionality of latent factors\n",
    "        - learning_rate: step size for gradient descent\n",
    "        - regularization: regularization term to prevent overfitting\n",
    "        - iterations: number of training iterations\n",
    "        - random_state: random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.factors = factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"\n",
    "        Train the BPR model on the user-item matrix\n",
    "        \n",
    "        Parameters:\n",
    "        - user_item_matrix: scipy sparse matrix with user-item interactions\n",
    "        \n",
    "        Returns:\n",
    "        - self\n",
    "        \"\"\"\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "        self.n_users, self.n_items = user_item_matrix.shape\n",
    "        \n",
    "        # Initialize latent factors\n",
    "        self.user_factors = np.random.normal(0, 0.1, (self.n_users, self.factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (self.n_items, self.factors))\n",
    "        \n",
    "        # Create a dictionary of items each user has interacted with\n",
    "        self.user_items = defaultdict(set)\n",
    "        for user, item in zip(*self.user_item_matrix.nonzero()):\n",
    "            self.user_items[user].add(item)\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.iterations):\n",
    "            # Sample triplets for training\n",
    "            for _ in range(user_item_matrix.nnz):\n",
    "                user, pos_item, neg_item = self._sample_triplet()\n",
    "                self._update_factors(user, pos_item, neg_item)\n",
    "            \n",
    "            # Print progress\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                print(f\"Completed iteration {iteration + 1}/{self.iterations}\")\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def _sample_triplet(self):\n",
    "        \"\"\"\n",
    "        Sample a (user, positive_item, negative_item) triplet for training\n",
    "        \n",
    "        Returns:\n",
    "        - user: sampled user index\n",
    "        - pos_item: a positive item (one the user has interacted with)\n",
    "        - neg_item: a negative item (one the user has not interacted with)\n",
    "        \"\"\"\n",
    "        # Sample a user who has rated at least one item\n",
    "        user = random.choice(list(self.user_items.keys()))\n",
    "        \n",
    "        # Sample a positive item (one the user has interacted with)\n",
    "        pos_item = random.choice(list(self.user_items[user]))\n",
    "        \n",
    "        # Sample a negative item (one the user has not interacted with)\n",
    "        neg_item = random.randint(0, self.n_items - 1)\n",
    "        while neg_item in self.user_items[user]:\n",
    "            neg_item = random.randint(0, self.n_items - 1)\n",
    "            \n",
    "        return user, pos_item, neg_item\n",
    "    \n",
    "    def _update_factors(self, user, pos_item, neg_item):\n",
    "        \"\"\"\n",
    "        Update model parameters based on a triplet\n",
    "        \n",
    "        Parameters:\n",
    "        - user: user index\n",
    "        - pos_item: positive item index\n",
    "        - neg_item: negative item index\n",
    "        \"\"\"\n",
    "        # Calculate prediction for positive and negative items\n",
    "        pos_pred = np.dot(self.user_factors[user], self.item_factors[pos_item])\n",
    "        neg_pred = np.dot(self.user_factors[user], self.item_factors[neg_item])\n",
    "        \n",
    "        # Calculate prediction difference\n",
    "        diff = neg_pred - pos_pred\n",
    "        \n",
    "        # Calculate sigmoid gradient\n",
    "        sigmoid = 1.0 / (1.0 + np.exp(-diff))\n",
    "        \n",
    "        # Calculate gradients\n",
    "        grad_user = sigmoid * (self.item_factors[neg_item] - self.item_factors[pos_item]) + self.regularization * self.user_factors[user]\n",
    "        grad_pos_item = sigmoid * (-self.user_factors[user]) + self.regularization * self.item_factors[pos_item]\n",
    "        grad_neg_item = sigmoid * self.user_factors[user] + self.regularization * self.item_factors[neg_item]\n",
    "        \n",
    "        # Update factors\n",
    "        self.user_factors[user] -= self.learning_rate * grad_user\n",
    "        self.item_factors[pos_item] -= self.learning_rate * grad_pos_item\n",
    "        self.item_factors[neg_item] -= self.learning_rate * grad_neg_item\n",
    "    \n",
    "    def recommend(self, user_id, n=10, exclude_seen=True):\n",
    "        \"\"\"\n",
    "        Generate item recommendations for a user\n",
    "        \n",
    "        Parameters:\n",
    "        - user_id: user index\n",
    "        - n: number of recommendations to generate\n",
    "        - exclude_seen: whether to exclude items the user has already interacted with\n",
    "        \n",
    "        Returns:\n",
    "        - list of n recommended item indices\n",
    "        \"\"\"\n",
    "        # Calculate predicted scores for all items\n",
    "        scores = np.dot(self.user_factors[user_id], self.item_factors.T)\n",
    "        \n",
    "        # If requested, exclude items the user has already interacted with\n",
    "        if exclude_seen and user_id in self.user_items:\n",
    "            seen_items = list(self.user_items[user_id])\n",
    "            scores[seen_items] = -np.inf\n",
    "            \n",
    "        # Get top n items by score\n",
    "        top_items = np.argsort(scores)[::-1][:n]\n",
    "        \n",
    "        return top_items\n",
    "    \n",
    "    def get_similar_items(self, item_id, n=10):\n",
    "        \"\"\"\n",
    "        Find items similar to a given item based on latent factors\n",
    "        \n",
    "        Parameters:\n",
    "        - item_id: item index\n",
    "        - n: number of similar items to retrieve\n",
    "        \n",
    "        Returns:\n",
    "        - list of n similar item indices\n",
    "        \"\"\"\n",
    "        # Calculate cosine similarity between the item and all other items\n",
    "        sim_scores = cosine_similarity([self.item_factors[item_id]], self.item_factors)[0]\n",
    "        \n",
    "        # Exclude the item itself\n",
    "        sim_scores[item_id] = -np.inf\n",
    "        \n",
    "        # Get top n items by similarity\n",
    "        similar_items = np.argsort(sim_scores)[::-1][:n]\n",
    "        \n",
    "        return similar_items\n",
    "\n",
    "\n",
    "def load_movielens_100k(path=\"ml-100k\"):\n",
    "    \"\"\"\n",
    "    Load the MovieLens 100K dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - path: path to the ML-100K dataset directory\n",
    "    \n",
    "    Returns:\n",
    "    - ratings_df: pandas DataFrame with columns 'user_id', 'item_id', 'rating'\n",
    "    - movie_df: pandas DataFrame with movie information\n",
    "    \"\"\"\n",
    "    # Load ratings\n",
    "    ratings_df = pd.read_csv(f\"{path}/u.data\", sep='\\t', \n",
    "                           names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "    \n",
    "    # Load movie information\n",
    "    movie_df = pd.read_csv(f\"{path}/u.item\", sep='|', encoding='latin-1',\n",
    "                          names=['item_id', 'title', 'release_date', 'video_release_date',\n",
    "                                 'IMDb_URL'] + [f'genre_{i}' for i in range(19)])\n",
    "    \n",
    "    return ratings_df, movie_df\n",
    "\n",
    "\n",
    "def create_user_item_matrix(ratings_df):\n",
    "    \"\"\"\n",
    "    Create a sparse user-item interaction matrix from ratings\n",
    "    \n",
    "    Parameters:\n",
    "    - ratings_df: pandas DataFrame with columns 'user_id', 'item_id', 'rating'\n",
    "    \n",
    "    Returns:\n",
    "    - user_item_matrix: scipy sparse matrix with user-item interactions\n",
    "    - user_mapping: dict mapping original user IDs to matrix indices\n",
    "    - item_mapping: dict mapping original item IDs to matrix indices\n",
    "    \"\"\"\n",
    "    # Create mappings from original IDs to matrix indices\n",
    "    user_ids = ratings_df['user_id'].unique()\n",
    "    item_ids = ratings_df['item_id'].unique()\n",
    "    \n",
    "    user_mapping = {user_id: i for i, user_id in enumerate(user_ids)}\n",
    "    item_mapping = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "    \n",
    "    # Map original IDs to matrix indices\n",
    "    rows = ratings_df['user_id'].map(user_mapping)\n",
    "    cols = ratings_df['item_id'].map(item_mapping)\n",
    "    \n",
    "    # Create binary matrix (1 if interaction exists, 0 otherwise)\n",
    "    data = np.ones(len(ratings_df))\n",
    "    user_item_matrix = csr_matrix((data, (rows, cols)), \n",
    "                                 shape=(len(user_mapping), len(item_mapping)))\n",
    "    \n",
    "    return user_item_matrix, user_mapping, item_mapping\n",
    "\n",
    "# Required for cosine similarity calculation\n",
    "def cosine_similarity(X, Y):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between samples in X and Y\n",
    "    \n",
    "    Parameters:\n",
    "    - X: array-like of shape (n_samples_X, n_features)\n",
    "    - Y: array-like of shape (n_samples_Y, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    - similarities: ndarray of shape (n_samples_X, n_samples_Y)\n",
    "    \"\"\"\n",
    "    # Normalize vectors\n",
    "    X_normalized = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    Y_normalized = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
    "    \n",
    "    # Replace NaN with 0 (for zero vectors)\n",
    "    X_normalized = np.nan_to_num(X_normalized)\n",
    "    Y_normalized = np.nan_to_num(Y_normalized)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    return np.dot(X_normalized, Y_normalized.T)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    diversity_reranking_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73526c96-c6f0-43a0-9bc8-b233e5e75a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
